________________________________A Survey on Automatic Text Summarization________________________________Dipanjan Das André F.T. Martins________________________________Language Technologies Institute Carnegie Mellon University {dipanjan, afm}@cs.cmu.edu________________________________November 21, 2007________________________________Abstract________________________________The increasing availability of online information has necessitated intensive research in the area of automatic text summarization within the Natural Language Processing (NLP) community. Over the past half a century, the problem has been addressed from many diﬀerent perspectives, in varying domains and using various paradigms. This survey intends to investigate some of the most relevant approaches both in the areas of single-document and multipledocument summarization, giving special emphasis to empirical methods and extractive techniques. Some promising approaches that concentrate on speciﬁc details of the summarization problem are also discussed. Special attention is devoted to automatic evaluation of summarization systems, as future research on summarization is strongly dependent on progress in this area.________________________________1 Introduction________________________________The subﬁeld of summarization has been investigated by the NLP community for nearly the last half century. Radev et al. (2002) deﬁne a summary as “a text that is produced from one or more texts, that conveys important information in the original text(s), and that is no longer than half of the original text(s) and usually signiﬁcantly less than that”. This simple deﬁnition captures three important aspects that characterize research on automatic summarization:________________________________• Summaries may be produced from a single document or multiple documents,________________________________• Summaries should preserve important information,________________________________• Summaries should be short.________________________________Even if we agree unanimously on these points, it seems from the literature that any attempt to provide a more elaborate deﬁnition for the task would result in disagreement within the community. In fact, many approaches diﬀer on the manner of their problem formulations. We start by introducing some common terms in the________________________________1________________________________summarization dialect: extraction is the procedure of identifying important sections of the text and producing them verbatim; abstraction aims to produce important material in a new way; fusion combines extracted parts coherently; and compression aims to throw out unimportant sections of the text (Radev et al., 2002). Earliest instances of research on summarizing scientiﬁc documents proposed paradigms for extracting salient sentences from text using features like word and phrase frequency (Luhn, 1958), position in the text (Baxendale, 1958) and key phrases (Edmundson, 1969). Various work published since then has concentrated on other domains, mostly on newswire data. Many approaches addressed the problem by building systems depending of the type of the required summary. While extractive summarization is mainly concerned with what the summary content should be, usually relying solely on extraction of sentences, abstractive summarization puts strong emphasis on the form, aiming to produce a grammatical summary, which usually requires advanced language generation techniques. In a paradigm more tuned to information retrieval (IR), one can also consider topic-driven summarization, that assumes that the summary content depends on the preference of the user and can be assessed via a query, making the ﬁnal summary focused on a particular topic. A crucial issue that will certainly drive future research on summarization is evaluation. During the last ﬁfteen years, many system evaluation competitions like TREC,1 DUC2 and MUC3 have created sets of training material and have established baselines for performance levels. However, a universal strategy to evaluate summarization systems is still absent. In this survey, we primarily aim to investigate how empirical methods have been used to build summarization systems. The rest of the paper is organized as follows: Section 2 describes single-document summarization, focusing on extractive techniques. Section 3 progresses to discuss the area of multi-document summarization, where a few abstractive approaches that pioneered the ﬁeld are also considered. Section 4 brieﬂy discusses some unconventional approaches that we believe can be useful in the future of summarization research. Section 5 elaborates a few evaluation techniques and describes some of the standards for evaluating summaries automatically. Finally, Section 6 concludes the survey.________________________________2 Single-Document Summarization________________________________Usually, the ﬂow of information in a given document is not uniform, which means that some parts are more important than others. The major challenge in summarization lies in distinguishing the more informative parts of a document from the less ones. Though there have been instances of research describing the automatic creation of abstracts, most work presented in the literature relies on verbatim extraction of sentences to address the problem of single-document summarization. In________________________________1See http://trec.nist.gov/. 2See http://duc.nist.gov/. 3See http://www.itl.nist.gov/iad/894.02/related projects/muc/proceedings/. muc 7 toc.html________________________________2________________________________this section, we describe some eminent extractive techniques. First, we look at early work from the 1950s and 60s that kicked oﬀ research on summarization. Second, we concentrate on approaches involving machine learning techniques published in the 1990s to today. Finally, we brieﬂy describe some techniques that use a more complex natural language analysis to tackle the problem.________________________________2.1 Early Work________________________________Most early work on single-document summarization focused on technical documents. Perhaps the most cited paper on summarization is that of (Luhn, 1958), that describes research done at IBM in the 1950s. In his work, Luhn proposed that the frequency of a particular word in an article provides an useful measure of its signiﬁcance. There are several key ideas put forward in this paper that have assumed importance in later work on summarization. As a ﬁrst step, words were stemmed to their root forms, and stop words were deleted. Luhn then compiled a list of content words sorted by decreasing frequency, the index providing a signiﬁcance measure of the word. On a sentence level, a signiﬁcance factor was derived that reﬂects the number of occurrences of signiﬁcant words within a sentence, and the linear distance between them due to the intervention of non-signiﬁcant words. All sentences are ranked in order of their signiﬁcance factor, and the top ranking sentences are ﬁnally selected to form the auto-abstract. Related work (Baxendale, 1958), also done at IBM and published in the same journal, provides early insight on a particular feature helpful in ﬁnding salient parts of documents: the sentence position. Towards this goal, the author examined 200 paragraphs to ﬁnd that in 85% of the paragraphs the topic sentence came as the ﬁrst one and in 7% of the time it was the last sentence. Thus, a naive but fairly accurate way to select a topic sentence would be to choose one of these two. This positional feature has since been used in many complex machine learning based systems. Edmundson (1969) describes a system that produces document extracts. His primary contribution was the development of a typical structure for an extractive summarization experiment. At ﬁrst, the author developed a protocol for creating manual extracts, that was applied in a set of 400 technical documents. The two features of word frequency and positional importance were incorporated from the previous two works. Two other features were used: the presence of cue words (presence of words like signiﬁcant, or hardly), and the skeleton of the document (whether the sentence is a title or heading). Weights were attached to each of these features manually to score each sentence. During evaluation, it was found that about 44% of the auto-extracts matched the manual extracts.________________________________2.2 Machine Learning Methods________________________________In the 1990s, with the advent of machine learning techniques in NLP, a series of seminal publications appeared that employed statistical techniques to produce document extracts. While initially most systems assumed feature independence and relied on naive-Bayes methods, others have focused on the choice of appropriate features and________________________________3________________________________on learning algorithms that make no independence assumptions. Other signiﬁcant approaches involved hidden Markov models and log-linear models to improve extractive summarization. A very recent paper, in contrast, used neural networks and third party features (like common words in search engine queries) to improve purely extractive single document summarization. We next describe all these approaches in more detail.________________________________2.2.1 Naive-Bayes Methods________________________________Kupiec et al. (1995) describe a method derived from Edmundson (1969) that is able to learn from data. The classiﬁcation function categorizes each sentence as worthy of extraction or not, using a naive-Bayes classiﬁer. Let s be a particular sentence, S the set of sentences that make up the summary, and F1, . . . , Fk the features. Assuming independence of the features:________________________________P(s ∈ S | F1, F2, ..Fk) = Qk i=1 P(Fi | s ∈ S) · P(s ∈ S) Qk i=1 P(Fi) (1)________________________________The features were compliant to (Edmundson, 1969), but additionally included the sentence length and the presence of uppercase words. Each sentence was given a score according to (1), and only the n top sentences were extracted. To evaluate the system, a corpus of technical documents with manual abstracts was used in the following way: for each sentence in the manual abstract, the authors manually analyzed its match with the actual document sentences and created a mapping (e.g. exact match with a sentence, matching a join of two sentences, not matchable, etc.). The auto-extracts were then evaluated against this mapping. Feature analysis revealed that a system using only the position and the cue features, along with the sentence length sentence feature, performed best. Aone et al. (1999) also incorporated a naive-Bayes classiﬁer, but with richer features. They describe a system called DimSum that made use of features like term frequency (tf ) and inverse document frequency (idf) to derive signature words.4________________________________The idf was computed from a large corpus of the same domain as the concerned documents. Statistically derived two-noun word collocations were used as units for counting, along with single words. A named-entity tagger was used and each entity was considered as a single token. They also employed some shallow discourse analysis like reference to same entities in the text, maintaining cohesion. The references were resolved at a very shallow level by linking name aliases within a document like “U.S.” to “United States”, or “IBM” for “International Business Machines”. Synonyms and morphological variants were also merged while considering lexical terms, the former being identiﬁed by using Wordnet (Miller, 1995). The corpora used in the experiments were from newswire, some of which belonged to the TREC evaluations.________________________________4Words that indicate key concepts in a document.________________________________4________________________________2.2.2 Rich Features and Decision Trees________________________________Lin and Hovy (1997) studied the importance of a single feature, sentence position. Just weighing a sentence by its position in text, which the authors term as the “position method”, arises from the idea that texts generally follow a predictable discourse structure, and that the sentences of greater topic centrality tend to occur in certain speciﬁable locations (e.g. title, abstracts, etc). However, since the discourse structure signiﬁcantly varies over domains, the position method cannot be deﬁned as naively as in (Baxendale, 1958). The paper makes an important contribution by investigating techniques of tailoring the position method towards optimality over a genre and how it can be evaluated for eﬀectiveness. A newswire corpus was used, the collection of Ziﬀ-Davis texts produced from the TIPSTER5 program; it consists of text about computer and related hardware, accompanied by a set of key topic words and a small abstract of six sentences. For each document in the corpus, the authors measured the yield of each sentence position against the topic keywords. They then ranked the sentence positions by their average yield to produce the Optimal Position Policy (OPP) for topic positions for the genre. Two kinds of evaluation were performed. Previously unseen text was used for testing whether the same procedure would work in a diﬀerent domain. The ﬁrst evaluation showed contours exactly like the training documents. In the second evaluation, word overlap of manual abstracts with the extracted sentences was measured. Windows in abstracts were compared with windows on the selected sentences and corresponding precision and recall values were measured. A high degree of coverage indicated the eﬀectiveness of the position method. In later work, Lin (1999) broke away from the assumption that features are independent of each other and tried to model the problem of sentence extraction using decision trees, instead of a naive-Bayes classiﬁer. He examined a lot of features and their eﬀect on sentence extraction. The data used in this work is a publicly available collection of texts, classiﬁed into various topics, provided by the TIPSTER-SUMMAC6 evaluations, targeted towards information retrieval systems. The dataset contains essential text fragments (phrases, clauses, and sentences) which must be included in summaries to answer some TREC topics. These fragments were each evaluated by a human judge. The experiments described in the paper are with the SUMMARIST system developed at the University of Southern California. The system extracted sentences from the documents and those were matched against human extracts, like most early work on extractive summarization. Some novel features were the query signature (normalized score given to sentences depending on number of query words that they contain), IR signature (the m most salient words in the corpus, similar to the signature words of (Aone et al., 1999)), numerical data (boolean value 1 given to sentences that contained a number in them), proper name (boolean value 1 given to sentences that contained a proper name in them), pronoun or adjective (boolean value 1 given to sentences________________________________5See http://www.itl.nist.gov/iaui/894.02/related_projects/tipster/. 6See http://www-nlpir.nist.gov/related_projects/tipster_summac/index.html.________________________________5________________________________that contained a pronoun or adjective in them), weekday or month (similar as previous feature) and quotation (similar as previous feature). It is worth noting that some features like the query signature are question-oriented because of the setting of the evaluation, unlike a generalized summarization framework. The author experimented with various baselines, like using only the positional feature, or using a simple combination of all features by adding their values. When evaluated by matching machine extracted and human extracted sentences, the decision tree classiﬁer was clearly the winner for the whole dataset, but for three topics, a naive combination of features beat it. Lin conjectured that this happened because some of the features were independent of each other. Feature analysis suggested that the IR signature was a valuable feature, corroborating the early ﬁndings of Luhn (1958).________________________________2.2.3 Hidden Markov Models________________________________In contrast with previous approaches, that were mostly feature-based and nonsequential, Conroy and O’leary (2001) modeled the problem of extracting a sentence from a document using a hidden Markov model (HMM). The basic motivation for using a sequential model is to account for local dependencies between sentences. Only three features were used: position of the sentence in the document (built into the state structure of the HMM), number of terms in the sentence, and likeliness of the sentence terms given the document terms.________________________________no 3 2 1 no no no________________________________Figure 1: Markov model to extract to three summary sentences from a document (Conroy and O’leary, 2001).________________________________The HMM was structured as follows: it contained 2s + 1 states, alternating between s summary states and s+1 nonsummary states. The authors allowed “hesitation” only in nonsummary states and “skipping next state” only in summary states. Figure 1 shows an example HMM with 7 nodes, corresponding to s = 3. Using the TREC dataset as training corpus, the authors obtained the maximum-likelihood estimate for each transition probability, forming the transition matrix estimate ˆ M, whose element (i, j) is the empirical probability of transitioning from state i to j. Associated with each state i was an output function, bi(O) = Pr(O | state i) where O is an observed vector of features. They made a simplifying assumption that the features are multivariate normal. The output function for each state was thus estimated by using the training data to compute the maximum likelihood estimate of its mean and covariance matrix. They estimated 2s+1 means, but assumed that all of the output functions shared a common covariance matrix. Evaluation was done________________________________6________________________________by comparing with human generated extracts.________________________________2.2.4 Log-Linear Models________________________________Osborne (2002) claims that existing approaches to summarization have always assumed feature independence. The author used log-linear models to obviate this assumption and showed empirically that the system produced better extracts than a naive-Bayes model, with a prior appended to both models. Let c be a label, s the item we are interested in labeling, fi the i-th feature, and λi the corresponding feature weight. The conditional log-linear model used by Osborne (2002) can be stated as follows:________________________________P(c | s) = 1________________________________Z(s) exp________________________________X________________________________i λifi(c, s)________________________________!________________________________, (2)________________________________where Z(s) = P c exp (P i λifi(c, s)). In this domain, there are only two possible labels: either the sentence is to be extracted or it is not. The weights were trained by conjugate gradient descent. The authors added a non-uniform prior to the model, claiming that a log-linear model tends to reject too many sentences for inclusion in a summary. The same prior was also added to a naive-Bayes model for comparison. The classiﬁcation took place as follows:________________________________label(s) = arg max c∈C P(c) · P(s, c) = arg max c∈C________________________________________________________________log P(c) + X________________________________i λifi(c, s)________________________________!________________________________. (3)________________________________The authors optimized the prior using the f2 score of the classiﬁer as an objective function on a part of the dataset (in the technical domain). The summaries were evaluated using the standard f2 score where f2 = 2pr________________________________p+r, where the precision and recall measures were measured against human generated extracts. The features included word pairs (pairs of words with all words truncated to ten characters), sentence length, sentence position, and naive discourse features like inside introduction or inside conclusion. With respect to f2 score, the log-linear model outperformed the naive-Bayes classiﬁer with the prior, exhibiting the former’s eﬀectiveness.________________________________2.2.5 Neural Networks and Third Party Features________________________________In 2001-02, DUC issued a task of creating a 100-word summary of a single news article. However, the best performing systems in the evaluations could not outperform the baseline with statistical signiﬁcance. This extremely strong baseline has been analyzed by Nenkova (2005) and corresponds to the selection of the ﬁrst n sentences of a newswire article. This surprising result has been attributed to the journalistic convention of putting the most important part of an article in the initial paragraphs. After 2002, the task of single-document summarization for newswire was dropped from DUC. Svore et al. (2007) propose an algorithm based on neural nets and the use of third party datasets to tackle the problem of extractive summarization, outperforming the baseline with statistical signiﬁcance.________________________________7________________________________The authors used a dataset containing 1365 documents gathered from CNN.com, each consisting of the title, timestamp, three or four human generated story highlights and the article text. They considered the task of creating three machine highlights. The human generated highlights were not verbatim extractions from the article itself. The authors evaluated their system using two metrics: the ﬁrst one concatenated the three highlights produced by the system, concatenated the three human generated highlights, and compared these two blocks; the second metric considered the ordering and compared the sentences on an individual level. Svore et al. (2007) trained a model from the labels and the features for each sentence of an article, that could infer the proper ranking of sentences in a test document. The ranking was accomplished using RankNet (Burges et al., 2005), a pair-based neural network algorithm designed to rank a set of inputs that uses the gradient descent method for training. For the training set, they used ROUGE-1 (Lin, 2004) to score the similarity of a human written highlight and a sentence in the document. These similarity scores were used as soft labels during training, contrasting with other approaches where sentences are “hard-labeled”, as selected or not. Some of the used features based on position or n-grams frequencies have been observed in previous work. However, the novelty of the framework lay in the use of features that derived information from query logs from Microsoft’s news search engine7 and Wikipedia8 entries. The authors conjecture that if a document sentence contained keywords used in the news search engine, or entities found in Wikipedia articles, then there is a greater chance of having that sentence in the highlight. The extracts were evaluated using ROUGE-1 and ROUGE-2, and showed statistically signiﬁcant improvements over the baseline of selecting the ﬁrst three sentences in a document.________________________________2.3 Deep Natural Language Analysis Methods________________________________In this subsection, we describe a set of papers that detail approaches towards singledocument summarization involving complex natural language analysis techniques. None of these papers solve the problem using machine learning, but rather use a set of heuristics to create document extracts. Most of these techniques try to model the text’s discourse structure. Barzilay and Elhadad (1997) describe a work that used considerable amount of linguistic analysis for performing the task of summarization. For a better understanding of their method, we need to deﬁne a lexical chain: it is a sequence of related words in a text, spanning short (adjacent words or sentences) or long distances (entire text). The authors’ method progressed with the following steps: segmentation of the text, identiﬁcation of lexical chains, and using strong lexical chains to identify the sentences worthy of extraction. They tried to reach a middle ground between (McKeown and Radev, 1995) and (Luhn, 1958) where the former relied on deep________________________________7See http://search.live.com/news. 8See http://en.wikipedia.org.________________________________8________________________________semantic structure of the text, while the latter relied on word statistics of the documents. The authors describe the notion of cohesion in text as a means of sticking together diﬀerent parts of the text. Lexical cohesion is a notable example where semantically related words are used. For example, let us take a look at the following sentence.9________________________________John bought a Jag. He loves the car. (4)________________________________Here, the word car refers to the word Jag in the previous sentence, and exempliﬁes lexical cohesion. The phenomenon of cohesion occurs not only at the word level, but at word sequences too, resulting in lexical chains, which the authors used as a source representation for summarization. Semantically related words and word sequences were identiﬁed in the document, and several chains were extracted, that form a representation of the document. To ﬁnd out lexical chains, the authors used Wordnet (Miller, 1995), applying three generic steps:________________________________1. Selecting a set of candidate words.________________________________2. For each candidate word, ﬁnding an appropriate chain relying on a relatedness criterion among members of the chains,________________________________3. If it is found, inserting the word in the chain and updating it accordingly.________________________________The relatedness was measured in terms of Wordnet distance. Simple nouns and noun compounds were used as starting point to ﬁnd the set of candidates. In the ﬁnal steps, strong lexical chains were used to create the summaries. The chains were scored by their length and homogeneity. Then the authors used a few heuristics to select the signiﬁcant sentences. In another paper, Ono et al. (1994) put forward a computational model of discourse for Japanese expository writings, where they elaborate a practical procedure for extracting the discourse rhetorical structure, a binary tree representing relations between chunks of sentences (rhetorical structure trees are used more intensively in (Marcu, 1998a), as we will see below). This structure was extracted using a series of NLP steps: sentence analysis, rhetorical relation extraction, segmentation, candidate generation and preference judgement. Evaluation was based on the relative importance of rhetorical relations. In the following step, the nodes of the rhetorical structure tree were pruned to reduce the sentence, keeping its important parts. Same was done for paragraphs to ﬁnally produce the summary. Evaluation was done with respect to sentence coverage and 30 editorial articles of a Japanese newspaper were used as the dataset. The articles had corresponding sets of key sentences and most important key sentences judged by human subjects. The key sentence coverage was about 51% and the most important key sentence coverage was 74%, indicating encouraging results. Marcu (1998a) describes a unique approach towards summarization that, unlike most other previous work, does not assume that the sentences in a document form a ﬂat sequence. This paper used discourse based heuristics with the traditional________________________________9Example from http://www.cs.ucd.ie/staff/jcarthy/home/Lex.html.________________________________9________________________________features that have been used in the summarization literature. The discourse theory used in this paper is the Rhetorical Structure Theory (RST) that holds between two non-overlapping pieces of text spans: the nucleus and the satellite. The author mentions that the distinction between nuclei and satellites comes from the empirical observation that the nucleus expresses what is more essential to the writer’s purpose than the satellite; and that the nucleus of a rhetorical relation is comprehensible independent of the satellite, but not vice versa. Marcu (1998b) describes the details of a rhetorical parser producing a discourse tree. Figure 2 shows an example discourse tree for a text example detailed in the paper. Once such a dis-________________________________Antithesis________________________________2________________________________Elaboration________________________________Elaboration________________________________2________________________________2________________________________Elaboration________________________________3________________________________Justification________________________________8________________________________Exemplification________________________________1 2 3 4 5 7 8________________________________4 5________________________________8 10________________________________9 10________________________________5 6________________________________Contrast________________________________Evidence________________________________Concession________________________________Figure 2: Example of a discourse tree from Marcu (1998a). The numbers in the nodes denote sentence numbers from the text example. The text below the number in selected nodes are rhetorical relations. The dotted nodes are SATELLITES and the normals ones are the NUCLEI.________________________________course structure is created, a partial ordering of important units can be developed from the tree. Each equivalence class in the partial ordering is derived from the new sentences at a particular level of the discourse tree. In Figure 2, we observe that sentence 2 is at the root, followed by sentence 8 in the second level. In the third level, sentence 3 and 10 are observed, and so forth. The equivalence classes are 2 > 8 > 3, 10 > 1, 4, 5, 7, 9 > 6. If it is speciﬁed that the summary should contain the top k% of the text, the ﬁrst k% of the units in the partial ordering can be selected to produce the summary. The author talks about a summarization system based just on this method in (Marcu, 1998b) and in one of his earlier papers. In this paper, he merged the discourse based heuristics with traditional heuristics. The metrics used were clustering based________________________________10________________________________metric (each node in the discourse tree was assigned a cluster score; for leaves the score was 0, for the internal nodes it was given by the similarity of the immediate children; discourse tree A was chosen to be better than B if its clustering score was higher), marker based metric (a discourse structure A was chosen to be better than a discourse structure B if A used more rhetorical relations than B), rhetorical clustering based technique (measured the similarity between salient units of two text spans), shape based metric (preferred a discourse tree A over B if A was more skewed towards the right than B), title based metric, position based metric, connectedness based metric (cosine similarity of an unit to all other text units, a discourse structure A was chosen to be better than B if its connectedness measure was more than B). A weighted linear combination of all these scores gave the score of a discourse structure. To ﬁnd the best combination of heuristics, the author computed the weights that maximized the F-score on the training dataset, which was constituted by newswire articles. To do this, he used a GSAT-like algorithm (Selman et al., 1992) that performed a greedy search in a seven dimensional space of the metrics. For a part of his corpus (the TREC dataset), a best F-score of 75.42% was achieved for the 10% summaries which was 3.5% higher than a baseline lead based algorithm, which was very encouraging.________________________________3 Multi-Document Summarization________________________________Extraction of a single summary from multiple documents has gained interest since mid 1990s, most applications being in the domain of news articles. Several Webbased news clustering systems were inspired by research on multi-document summarization, for example Google News,10 Columbia NewsBlaster,11 or News In Essence.12________________________________This departs from single-document summarization since the problem involves multiple sources of information that overlap and supplement each other, being contradictory at occasions. So the key tasks are not only identifying and coping with redundancy across documents, but also recognizing novelty and ensuring that the ﬁnal summary is both coherent and complete. The ﬁeld seems to have been pioneered by the NLP group at Columbia University (McKeown and Radev, 1995), where a summarization system called SUMMONS13________________________________was developed by extending already existing technology for template-driven message understanding systems. Although in that early stage multi-document summarization was mainly seen as a task requiring substantial capabilities of both language interpretation and generation, it later gained autonomy, as people coming from different communities added new perspectives to the problem. Extractive techniques have been applied, making use of similarity measures between pairs of sentences. Approaches vary on how these similarities are used: some identify common themes through clustering and then select one sentence to represent each cluster (McKeown________________________________10See http://news.google.com. 11See http://newsblaster.cs.columbia.edu. 12See http://NewsInEssence.com. 13SUMMarizing Online NewS articles.________________________________11________________________________et al., 1999; Radev et al., 2000), others generate a composite sentence from each cluster (Barzilay et al., 1999), while some approaches work dynamically by including each candidate passage only if it is considered novel with respect to the previous included passages, via maximal marginal relevance (Carbonell and Goldstein, 1998). Some recent work extends multi-document summarization to multilingual environments (Evans, 2005). The way the problem is posed has also varied over time. While in some publications it is claimed that extractive techniques would not be eﬀective for multidocument summarization (McKeown and Radev, 1995; McKeown et al., 1999), some years later that claim was overturned, as extractive systems like MEAD14 (Radev et al., 2000) achieved good performance in large scale summarization of news articles. This can be explained by the fact that summarization systems often distinguish among themselves about what their goal actually is. While some systems, like SUMMONS, are designed to work in strict domains, aiming to build a sort of brieﬁng that highlights diﬀerences and updates accross diﬀerent news reports, putting much emphasis on how information is presented to the user, others, like MEAD, are large scale systems that intend to work in general domains, being more concerned with information content rather than form. Consequently, systems of the former kind require a strong eﬀort on language generation to produce a grammatical and coherent summary, while latter systems are probably more close to the information retrieval paradigm. Abstractive systems like SUMMONS are diﬃcult to replicate, as they heavily rely on the adaptation of internal tools to perform information extraction and language generation. On the other hand, extractive systems are generally easy to implement from scratch, and this makes them appealing when sophisticated NLP tools are not available.________________________________3.1 Abstraction and Information Fusion________________________________As far as we know, SUMMONS (McKeown and Radev, 1995; Radev and McKeown, 1998) is the ﬁrst historical example of a multi-document summarization system. It tackles single events about a narrow domain (news articles about terrorism) and produces a brieﬁng merging relevant information about each event and how reports by diﬀerent news agencies have evolved over time. The whole thread of reports is then presented, as illustrated in the following example of a “good” summary:________________________________“In the afternoon of February 26, 1993, Reuters reported that a suspect bomb killed at least ﬁve people in the World Trade Center. However, Associated Press announced that exactly ﬁve people were killed in the blast. Finally, Associated Press announced that Arab terrorists were possibly responsible for the terrorist act.”________________________________Rather than working with raw text, SUMMONS reads a database previously built by a template-based message understanding system. A full multi-document________________________________14Available for download at http://www.summarization.com/mead/.________________________________12________________________________summarizer is built by concatenating the two systems, ﬁrst processing full text as input and ﬁlling template slots, and then synthesizing a summary from the extracted information. The architecture of SUMMONS consists of two major components: a content planner that selects the information to include in the summary through combination of the input templates, and a linguistic generator that selects the right words to express the information in grammatical and coherent text. The latter component was devised by adapting existing language generation tools, namely the FUF/SURGE system15. Content planning, on the other hand, is made through summary operators, a set of heuristic rules that perform operations like “change of perspective”, “contradiction”, “reﬁnement”, etc. Some of these operations require resolving conﬂicts, i.e., contradictory information among diﬀerent sources or time instants; others complete pieces of information that are included in some articles and not in others, combining them into a single template. At the end, the linguistic generator gathers all the combined information and uses connective phrases to synthesize a summary. While this framework seems promising when the domain is narrow enough so that the templates can be designed by hand, a generalization for broader domains would be problematic. This was improved later by McKeown et al. (1999) and Barzilay et al. (1999), where the input is now a set of related documents in raw text, like those retrieved by a standard search engine in response to a query. The system starts by identifying themes, i.e., sets of similar text units (usually paragraphs). This is formulated as a clustering problem. To compute a similarity measure between text units, these are mapped to vectors of features, that include single words weighted by their TF-IDF scores, noun phrases, proper nouns, synsets from the Wordnet database and a database of semantic classes of verbs. For each pair of paragraphs, a vector is computed that represents matches on the diﬀerent features. Decision rules that were learned from data are then used to classify each pair of text units either as similar or dissimilar; this in turn feeds a subsequent algorithm that places the most related paragraphs in the same theme. Once themes are identiﬁed, the system enters its second stage: information fusion. The goal is to decide which sentences of a theme should be included in the summary. Rather than just picking a sentence that is a group representative, the authors propose an algorithm which compares and intersects predicate argument structures of the phrases within each theme to determine which are repeated often enough to be included in the summary. This is done as follows: ﬁrst, sentences are parsed through Collins’ statistical parser (Collins, 1999) and converted into dependency trees, which allows capturing the predicate-argument structure and identify functional roles. Determiners and auxiliaries are dropped; Fig. 3 shows a sentence representation. The comparison algorithm then traverses these dependency trees recursively, adding identical nodes to the output tree. Once full phrases (a verb with at least two constituents) are found, they are marked to be included in the summary. If two________________________________15FUF, SURGE, and other tools developed by the Columbia NLP group are available at http://www1.cs.columbia.edu/nlp/tools.cgi.________________________________13________________________________and Kan 1998]. We match two verbs that share the same semantic class in this classiﬁ cation.________________________________In addition to the above primitive features that all com-________________________________pare single items from each text unit, we use composite features that combine pairs of primitive features. Our composite features impose particular constraints on the order of the two elements in the pair, on the maximum distance between the two elements, and on the syntactic classes that the two elements come from. They can vary from a simple combination (e.g., “two text units must share two words to be similar”) to complex cases with many conditions (e.g., “two text units must have matching noun phrases that appear in the same order and with relative difference in position no more than ﬁ ve”). In this manner, we capture information on how similarly related elements are spaced out in the two text units, as well as syntactic information on word combinations. Matches on composite features indicate combined evidence for the similarity of the two units.________________________________To determine whether the units match overall, we employ________________________________a machine learning algorithm [Cohen 1996] that induces decision rules using the features that really make a difference. A set of pairs of units already marked as similar or not by a human is used for training the classiﬁ er. We have manually marked a set of 8,225 paragraph comparisons from the TDT corpus for training and evaluating our similarity classiﬁ er.________________________________For comparison, we also use an implementation of the________________________________TF*IDF method which is standard for matching texts in information retrieval. We compute the total frequency (TF) of words in each text unit and the number of units in our training set each word appears in (DF, or document frequency). Then each text unit is represented as a vector of TF*IDF scores, calculated as________________________________TF(wordi) · log Total number of units________________________________DF(wordi)________________________________Similarity between text units is measured by the cosine of the angle between the corresponding two vectors (i.e., the normalized inner product of the two vectors), and the optimal value of a threshold for judging two units as similar is computed from the training set.________________________________After all pairwise similarities between text units have________________________________been calculated, we utilize a clustering algorithm to identify themes. As a paragraph may belong to multiple themes, most standard clustering algorithms, which partition their input set, are not suitable for our task. We use a greedy, one-pass algorithm that ﬁ rst constructs groups from the most similar paragraphs, seeding the groups with the fully connected subcomponents of the graph that the similarity relationship induces over the set of paragraphs, and then places additional paragraphs within a group if the fraction of the members of the group they are similar to exceeds a preset threshold.________________________________Language Generation________________________________Given a group of similar paragraphs—a theme—the problem is to create a concise and ﬂuent fusion of information in this theme, reﬂecting facts common to all paragraphs. A straightforward method would be to pick a representative________________________________subject________________________________class: noun________________________________27________________________________class: cardinal bombing________________________________class: noun________________________________McVeigh with________________________________class: preposition________________________________definite: yes________________________________charge________________________________class: verb voice :passive polarity: + tense: past________________________________Figure 4: Dependency grammar representation of the sentence “McVeigh, 27, was charged with the bombing”.________________________________sentence that meets some criteria (e.g., a threshold number of common content words). In practice, however, any representative sentence will usually include embedded phrase(s) containing information that is not common to all sentences in the theme. Furthermore, other sentences in the theme often contain additional information not presented in the representative sentence. Our approach, therefore, uses intersection among theme sentences to identify phrases common to most paragraphs and then generates a new sentence from identiﬁ ed phrases.________________________________Intersection among Theme Sentences________________________________Intersection is carried out in the content planner, which uses a parser for interpreting the input sentences, with our new work focusing on the comparison of phrases. Theme sentences are ﬁ rst run through a statistical parser[Collins 1996] and then, in order to identify functional roles (e.g., subject, object), are converted to a dependency grammar representation [Kittredge and Mel’ˇcuk 1983], which makes predicateargument structure explicit.________________________________We developed a rule-based component to produce func-________________________________tional roles, which transforms the phrase-structure output of Collins’ parser to dependency grammar; function words (determiners and auxiliaries) are eliminated from the tree and corresponding syntactic features are updated. An example of a theme sentence and its dependency grammar representation are shown in Figure 4. Each non-auxiliary word in the sentence has a node in the representation, and this node is connected to its direct dependents.________________________________The comparison algorithm starts with all subtrees rooted________________________________at verbs from the input dependency structure, and traverses them recursively: if two nodes are identical, they are added to the output tree, and their children are compared. Once a full phrase (verb with at least two constituents) has been found, it is conﬁ rmed for inclusion in the summary.________________________________Difﬁ culties arise when two nodes are not identical, but are________________________________similar. Such phrases may be paraphrases of each other and still convey essentially the same information. Since theme sentences are a priori close semantically, this signiﬁ cantly________________________________Figure 3: Dependency tree representing the sentence “McVeigh, 27, was charged with the bombing” (extracted from (McKeown et al., 1999)).________________________________phrases, rooted at some node, are not identical but yet similar, the hypothesis that they are paraphrases of each other is considered; to take this into account, corpusdriven paraphrasing rules are written to allow paraphrase intersection.16 Once the summary content (represented as predicate-argument structures) is decided, a grammatical text is generated by translating those structures into the arguments expected by the FUF/SURGE language generation system.________________________________3.2 Topic-driven Summarization and MMR________________________________Carbonell and Goldstein (1998) made a major contribution to topic-driven summarization by introducing the maximal marginal relevance (MMR) measure. The idea is to combine query relevance with information novelty; it may be applicable in several tasks ranging from text retrieval to topic-driven summarization. MMR simultaneously rewards relevant sentences and penalizes redundant ones by considering a linear combination of two similarity measures. Let Q be a query or user proﬁle and R a ranked list of documents retrieved by a search engine. Consider an incremental procedure that selects documents, one at a time, and adds them to a set S. So let S be the set of already selected documents in a particular step, and R \ S the set of yet unselected documents in R. For each candidate document Di ∈ R \ S, its marginal relevance MR(Di) is computed as:________________________________MR(Di) := λSim1(Di, Q) − (1 − λ) max Dj∈S Sim2(Di, Dj) (5)________________________________where λ is a parameter lying in [0, 1] that controls the relative importance given to relevance versus redundancy. Sim1 and Sim2 are two similarity measures; in the________________________________16A full description of the kind of paraphrasing rules used can be found in (Barzilay et al., 1999). Examples are: ordering of sentence components, main clause vs. relative clause, realization in diﬀerent syntactic categories (e.g. classiﬁer vs. apposition), change in grammatical features (active/passive, time, number, etc.), head omission, transformation from one POS to another, using semantically related words (e.g. synonyms), etc.________________________________14________________________________experiments both were set to the standard cosine similarity traditionally used in the vector space model, Sim1(x, y) = Sim2(x, y) = ⟨x,y⟩________________________________∥x∥·∥y∥. The document achieving the highest marginal relevance, DMMR = arg maxDi∈R\S MR(Di), is then selected, i.e., added to S, and the procedure continues until a maximum number of documents are selected or a minimum relevance threshold is attained. Carbonell and Goldstein (1998) found experimentally that choosing dynamically the value of λ turns out to be more eﬀective than keeping it ﬁxed, namely starting with small values (λ ≈ 0.3) to give more emphasis to novelty, and then increasing it (λ ≈ 0.7) to focus on the most relevant documents. To perform summarization, documents can be ﬁrst segmented into sentences or paragraphs, and after a query is submitted, the MMR algorithm can be applied followed by a selection of the top ranking passages, reordering them as they appeared in the original documents, and presenting the result as the summary. One of the attractive points in using MMR for summarization is its topic-oriented feature, through its dependency on the query Q, which makes it particularly appealing to generate summaries according to a user proﬁle: as the authors claim, “a diﬀerent user with diﬀerent information needs may require a totally diﬀerent summary of the same document.” This assertion was not being taken into account by previous multi-document summarization systems.________________________________3.3 Graph Spreading Activation________________________________Mani and Bloedorn (1997) describe an information extraction framework for summarization, a graph-based method to ﬁnd similarities and dissimilarities in pairs of documents. Albeit no textual summary is generated, the summary content is represented via entities (concepts) and relations that are displayed respectively as nodes and edges of a graph. Rather than extracting sentences, they detect salient regions of the graph via a spreading activation technique.17________________________________This approach shares with the method described in Section 3.2 the property of being topic-driven; there is an additional input that stands for the topic with respect to which the summary is to be generated. The topic is represented through a set of entry nodes in the graph. A document is represented as a graph as follows: each node represents the occurrence of a single word (i.e., one word together with its position in the text). Each node can have several kinds of links: adjacency links (ADJ) to adjacent words in the text, SAME links to other occurrences of the same word, and ALPHA links encoding semantic relationships captured through Wordnet and NetOwl18. Besides these, PHRASE links tie together sequences of adjacent nodes which belong to the same phrase, and NAME and COREF links stand for co-referential name occurrences; Fig. 4 shows some of these links. Once the graph is built, topic nodes are identiﬁed by stem comparison and become the entry nodes. A search for semantically related text is then propagated from these to the other nodes of the graph, in a process called spreading activation. Salient________________________________17The name “spreading activation” is borrowed from a method used in information retrieval (Salton and Buckley, 1988) to expand the search vocabulary. 18See http://www.netowl.com.________________________________15________________________________1.39: Aoki, the Japanese ambassador, said in telephone calls to________________________________Fujimori. Japanesebroadcaster NHK that the rebels wanted to talk directly to________________________________1.43:According to some estimates, only a couple hundred armed followers remain.________________________________2.19 They are freeing u________________________________not doing us any harm,"________________________________...________________________________2.27:Although the MRTA early days in the mid-198________________________________give to the poor, it lost pu turning increasingly to ki________________________________billion in damage to the c since 1980.________________________________and drug activities. 2.28:________________________________Peru have cost at least 3________________________________...________________________________close ties with Japan.________________________________1.33: Among the hostages were Japanese Ambassador Morihisa Aoki and the ambassadors of Brazil, Bolivia, Cuba, Canada, South Korea,________________________________...________________________________...________________________________...________________________________2.26:The MRTA called T "Breaking The Silence."________________________________1.32: President Alberto Fujimori, who is of Japanese ancestry, has had________________________________Germany, Austria and Venezuela. Hood-style movement tha________________________________j________________________________negotiations with the gov dawn on Wednesday. ...________________________________... 2.22:The attack was a ma Fujimori’s government, w virtual victory in a 16-yea rebels belonging to the M and better-known Maoist________________________________... 1.28:Many leaders of the Tupac Amaru which is smaller than Peru’s________________________________was captured in June 1992 and is serving a life sentence, as is his________________________________us: ‘Don’t lift your heads up or you will be shot." 1.19:________________________________hostages," a rebel who did not give his name told  a local radio station in a telephone call from inside the compound.________________________________"The guerillas stalked around the residence grounds threatening________________________________lieutenant, Peter Cardenas.________________________________1.25: "We are clear: the liberation of all our comrades, or we die with all the________________________________1.30:Other top commanders conceded defeat July 1993. and surrendered in________________________________COREF________________________________Maoist Shining Path movement are in jail. 1.29:Its chief, Victor Polay,________________________________ADJ________________________________1.38:Fujimori whose sister was among the an emergency cabinet meeting today.________________________________hostages released, called________________________________ALPHA________________________________ADJ________________________________, the rebels threatened to kill the remaining________________________________captives. 1.24:Early Wednesday________________________________Figure 5: Texts of two related articles. The top 5 salient sentences containing common words in bold face; likewise, the top 5 salient sentences containing unique words have th________________________________Figure 4: Examples of nodes and links in the graph for a particular sentence (detail extracted from from a ﬁgure in (Mani and Bloedorn, 1997)).________________________________words and phrases are initialized according to their TF-IDF score. The weight of neighboring nodes depends on the node link traveled and is an exponentially decaying function of the distance of the traversed path. Traveling within a sentence is made cheaper than across sentence boundaries, which in turn is cheaper than across paragraph boundaries. Given a pair of document graphs, common nodes are identiﬁed either by sharing the same stem or by being synonyms. Analogously, diﬀerence nodes are those that are not common. For each sentence in both documents, two scores are computed: one score that reﬂects the presence of common nodes, which is computed as the average weight of these nodes; and another score that computes instead the average weights of diﬀerence nodes. Both scores are computed after spreading activation. In the end, the sentences that have higher common and different scores are highlighted, the user being able to specify the maximal number of common and diﬀerent sentences to control the output. In the future, the authors expect to use these structure to actually compose abstractive summaries, rather than just highlighting pieces of text.________________________________3.4 Centroid-based Summarization________________________________Although clustering techniques were already being employed by McKeown et al. (1999) and Barzilay et al. (1999) for identiﬁcation of themes, Radev et al. (2000) pioneered the use of cluster centroids to play a central role in summarization. A full description of the centroid-based approach that underlies the MEAD system can be found in (Radev et al., 2004); here we sketch brieﬂy the main points. Perhaps the most appealing feature is the fact that it does not make use of any language generation module, unlike most previous systems. All documents are modeled as bags-of-words. The system is also easily scalable and domain-independent. The ﬁrst stage consists of topic detection, whose goal is to group together news articles that describe the same event. To accomplish this task, an agglomerative clustering algorithm is used that operates over the TF-IDF vector representations of the documents, successively adding documents to clusters and recomputing the________________________________16________________________________centroids according to________________________________cj =________________________________P d∈Cj ˜d________________________________|Cj| (6)________________________________where cj is the centroid of the j-th cluster, Cj is the set of documents that belong to that cluster, its cardinality being |Cj|, and ˜d is a “truncated version” of d that vanishes on those words whose TF-IDF scores are below a threshold. Centroids can thus be regarded as pseudo-documents that include those words whose TFIDF scores are above a threshold in the documents that constitute the cluster. Each event cluster is a collection of (typically 2 to 10) news articles from multiple sources, chronologically ordered, describing an event as it develops over time. The second stage uses the centroids to identify sentences in each cluster that are central to the topic of the entire cluster. In (Radev et al., 2000), two metrics are deﬁned that resemble the two summands in the MMR (see Section 3.2): clusterbased relative utility (CBRU) and cross-sentence informational subsumption (CSIS). The ﬁrst accounts for how relevant a particular sentence is to the general topic of the entire cluster; the second is a measure of redundancy among sentences. Unlike MMR, these metrics are not query-dependent. Given one cluster C of documents segmented into n sentences, and a compression rate R, a sequence of nR sentences are extracted in the same order as they appear in the original documents, which in turn are ordered chronologically. The selection of the sentences is made by approximating their CBRU and CSIS.19 For each sentence si, three diﬀerent features are used:________________________________• Its centroid value (Ci), deﬁned as the sum of the centroid values of all the words in the sentence,________________________________• A positional value (Pi), that is used to make leading sentences more important. Let Cmax be the centroid value of the highest ranked sentence in the document. Then Pi = n−i+1________________________________n Cmax.________________________________• The ﬁrst-sentence overlap (Fi), deﬁned as the inner product between the word occurrence vector of sentence i and that of the ﬁrst sentence of the document.________________________________The ﬁnal score of each sentence is a combination of the three scores above minus a redundancy penalty (Rs) for each sentence that overlaps highly ranked sentences.________________________________3.5 Multilingual Multi-document Summarization________________________________Evans (2005) addresses the task of summarizing documents written in multiple languages; this had already been sketched by Hovy and Lin (1999). Multilingual summarization is still at an early stage, but this framework looks quite useful for newswire applications that need to combine information from foreign news agencies. Evans (2005) considered the scenario where there is a preferred language in which the summary is to be written, and multiple documents in the preferred and________________________________19The two metrics are used directly for evaluation (see (Radev et al., 2004) for more details).________________________________17________________________________in foreign languages are available. In their experiments, the preferred language was English and the documents are news articles in English and Arabic. The rationale is to summarize the English articles without discarding the information contained in the Arabic documents. The IBM’s statistical machine translation system is ﬁrst applied to translate the Arabic documents to English. Then a search is made, for each translated text unit, to see whether there is a similar sentence or not in the English documents. If so, and if the sentence is found relevant enough to be included in the summary, the similar English sentence is included instead of the Arabic-to-English translation. This way, the ﬁnal summary is more likely to be grammatical, since machine translation is known to be far from perfect. On the other hand, the result is also expected to have higher coverage than using just the English documents, since the information contained in the Arabic documents can help to decide about the relevance of each sentence. In order to measure similarity between sentences, a tool named SimFinder20 was employed: this is a tool for clustering text based on similarity over a variety of lexical and syntactic features using a log-linear regression model.________________________________4 Other Approaches to Summarization________________________________This section describes brieﬂy some unconventional approaches that, rather than aiming to build full summarization systems, investigate some details that underlie the summarization process, and that we conjecture to have a role to play in future research on this ﬁeld.________________________________4.1 Short Summaries________________________________Witbrock and Mittal (1999) claim that extractive summarization is not very powerful in that the extracts are not concise enough when very short summaries are required. They present a system that generated headline style summaries. The corpus used in this work was newswire articles from Reuters and the Associated Press, publicly available at the LDC21. The system learned statistical models of the relationship between source text units and headline units. It attempted to model both the order and the likelihood of the appearance of tokens in the target documents. Both the models, one for content selection and the other for surface realization were used to co-constrain each other during the search in the summary generation task. For content selection, the model learned a translation model between a document and its summary (Brown et al., 1993). This model in the simplest case can be thought as a mapping between a word in the document and the likelihood of some word appearing in the summary. To simplify the model, the authors assumed that the probability of a word appearing in a summary is independent of its structure. This mapping boils down to the fact that the probability of a particular summary________________________________20See http://www1.cs.columbia.edu/nlp/tools.cgi#SimFinder. 21See http://ldc.upenn.edu.________________________________18________________________________candidate is the product of the probabilities of the summary content and that content being expressed using a particular structure. The surface realization model used was a bigram model. Viterbi beam search was used to eﬃciently ﬁnd a near-optimal summary. The Markov assumption was violated by using backtracking at every state to strongly discourage paths that repeated terms, since bigrams that start repeating often seem to pathologically overwhelm the search otherwise. To evaluate the system, the authors compared its output against the actual headlines for a set of input newswire stories. Since phrasing could not be compared, they compared the generated headlines against the actual headlines, as well as the top ranked summary sentence of the story. Since the system did not have a mechanism to determine the optimal length of a headline, six headlines for each story were generated, ranging in length from 4 to 10 words and they measured the term-overlap between each of the generated headlines and the test. For headline length 4, there was 0.89 overlap in the headline and there was 0.91 overlap amongst the top scored sentence, indicating useful results.________________________________4.2 Sentence Compression________________________________Knight and Marcu (2000) introduced a statistical approach to sentence compression. The authors believe that understanding the simpler task of compressing a sentence may be a fruitful ﬁrst step to later tackle the problems of single and multi-document summarization. Sentence compression is deﬁned as follows: given a sequence of words W = w1w2 . . . wn that constitute a sentence, ﬁnd a subsequence wi1wi2 . . . wik, with 1 ≤ i1 < i2 < . . . ik ≤ n, that is a compressed version of W. Note that there are 2n possibilities of output. Knight and Marcu (2000) considered two diﬀerent approaches: one that is inspired by the noisy-channel model, and another one based on decision trees. Due to its simplicity and elegance, we describe the ﬁrst approach here. The noisy-channel model considers that one starts with a short summary s, drawn according to the source model P(s), which is then subject to channel noise to become the full sentence t, in a process guided by the channel model P(t|s). When the string t is observed, one wants to recover the original summary according to:________________________________ˆs = arg max s P(s|t) = arg max s P(s)P(t|s). (7)________________________________This model has the advantage of decoupling the goals of producing a short text that looks grammatical (incorporated in the source model) and of preserving important information (which is done through the channel model). In (Knight and Marcu, 2000), the source and channel models are simple models inspired by probabilistic context-free grammars (PCFGs). The following probability mass functions are deﬁned over parse trees rather than strings: Ptree(s), the probability of a parse tree that generates s, and Pexpand tree(t|s), the probability that a small parse tree that generates s is expanded to a longer one that generates t.________________________________19________________________________The sentence t is ﬁrst parsed by using Collins’ parser (Collins, 1999). Then, rather than computing Ptree(s) over all the 2n hypotheses for s, which would be exponential in the sentence length, a shaded-forest structure is used: the parse tree of t is traversed and the grammar (learned from the Penn Treebank22) is used to check recursively which nodes may be removed from each production in order to achieve another valid production. This algorithm allows to compute eﬃciently Ptree(s) and Pexpand tree(t|s) for all possible grammatical summaries s. Conceptually, the noisy channel model works the other way around: summaries are the original strings that are expanded via expansion templates. Expansion operations have the eﬀect of decreasing the probability Pexpand tree(t|s). The probabilities Ptree(s) and Pexpand tree(t|s) consist in the usual factorized expression for PCFGs times a bigram distribution over the leaves of the tree (i.e. the words). In the end, the log probability is (heuristically) divided by the length of the sentence s in order not to penalize excessively longer sentences (this is done commonly in speech recognition). More recently, Daumé III and Marcu (2002) extended this approach to document compression by using rhetorical structure theory as in Marcu (1998a), where the entire document is represented as a tree, hence allowing not only to compress relevant sentences, but also to drop irrelevant ones. In this framework, Daumé III and Marcu (2004) employed kernel methods to decide for each node in the tree whether or not it should be kept.________________________________4.3 Sequential document representation________________________________We conclude this section by mentioning some recent work that concerns document representation, with applications in summarization. In the bag-of-words representation (Salton et al., 1975) each document is represented as a sparse vector in a very large Euclidean space, indexed by words in the vocabulary V . A well-known technique in information retrieval to capture word correlation is latent semantic indexing (LSI), that aims to ﬁnd a linear subspace of dimension k ≤ |V | where documents may be approximately represented by their projections. These classical approaches assume by convenience that Euclidean geometry is a proper model for text documents. As an alternative, Gous (1999) and Hall and Hofmann (2000) used the framework of information geometry (Amari and Nagaoka, 2001) to generalize LSI to the multinomial manifold, which can be identiﬁed with the probability simplex________________________________Pn−1 =________________________________(________________________________x ∈ Rn |________________________________n X________________________________i=1 xi = 1, xi ≥ 0 for i = 1, . . . , n________________________________)________________________________. (8)________________________________Instead of ﬁnding a linear subspace, as in the Euclidean case, they learn a submanifold of Pn−1. To illustrate this idea, Gous (1999) split a book (Machiavelli’s The Prince) into several text blocks (its numbered pages), considered each page as a point in P|V |−1, and projected data into a 2-dimensional submanifold. The result is________________________________22See http://www.cis.upenn.edu/~treebank/.________________________________20________________________________the representation of the book as a sequential path in R2, tracking the evolution of the subject matter of the book over the course of its pages (see Fig. 5). Inspired by________________________________Figure 5: The 113 pages of The Prince projected onto a 2-dimensional space (extracted from (Gous, 1999)). The inﬂection around page 85 reﬂects a real change in the subject matter, where the book shifts from political theory to a more biographical discourse.________________________________this framework, Lebanon et al. (2007) suggested representing a document as a simplicial curve (i.e. a curve in the probability simplex), yielding the locally weighted bag-of-words (lowbow) model. According to this representation, a length-normalized document is a function x : [0, 1] × V → R+ such that X________________________________wj∈V x(t, wj) = 1, for any t ∈ [0, 1]. (9)________________________________We can regard the document as a continuous signal, and x(t, wj) as expressing the relevance of word wj at instant t. This generalizes both the pure sequential representation and the (global) bag-of-words model. Let y = (y1, . . . , yn) ∈ V n be a n-length document. The pure sequential representation of y arises by deﬁning x = xseq with:________________________________xseq(t, wj) =  1, if wj = y⌈tn⌉ 0, if wj ̸= y⌈tn⌉, (10)________________________________where ⌈a⌉ denotes the smallest integer greater than a. The global bag-of-words representation of x corresponds to deﬁning x = xbow, where________________________________xbow(µ, wj) = Z 1________________________________0 xseq(t, wj)dt, µ ∈ [0, 1], j = 1, . . . , |V |. (11)________________________________In this case, the curve degenerates into a single point in the simplex, which is the maximum likelihood estimate of the multinomial parameters. An intermediate________________________________21________________________________representation arises by smoothing (10) via a function fµ,σ : [0, 1] → R++, where µ ∈ [0, 1] and σ ∈ R++ are respectively a location and a scale parameter. An example of such a smoothing function is the truncated Gaussian deﬁned in [0, 1] and normalized. This allows deﬁning the lowbow representation at µ of the n-lenght document (y1, . . . , yn) ∈ V n as the function x : [0, 1] × V → R+ such that:________________________________x(µ, wj) = Z 1________________________________0 xseq(t, wj)fµ,σ(t)dt. (12)________________________________The scale of the smoothing function controls the amount of locality/globality in the document representation (see Fig. 6): when σ → ∞ we recover the global bow representation (11); when σ → 0, we approach the pure sequential representation (10).________________________________Figure 6: The lowbow representation of a document with |V | = 3, for several values of the scale parameter σ (extracted from (Lebanon, 2006)).________________________________Representing a document as a simplicial curve allows us to characterize geometrically several properties of the document. For example, the tangent vector ﬁeld along the curve describes sequential “topic trends” and their change; the curvature measures the amount of wigglyness or deviation from a geodesic path. This properties can be useful for tasks like text segmentation or summarization; for example plotting the velocity of the curve || ˙x(µ)|| along time oﬀers a visualization of the document where local maxima tend to correspond to topic boundaries (see (Lebanon et al., 2007) for more information).________________________________22________________________________5 Evaluation________________________________Evaluating a summary is a diﬃcult task because there does not exist an ideal summary for a given document or set of documents. From papers surveyed in the previous sections and elsewhere in literature, it has been found that agreement between human summarizers is quite low, both for evaluating and generating summaries. More than the form of the summary, it is diﬃcult to evaluate the summary content. Another important problem in summary evaluation is the widespread use of disparate metrics. The absence of a standard human or automatic evaluation metric makes it very hard to compare diﬀerent systems and establish a baseline. This problem is not present in other NLP problems, like parsing. Besides this, manual evaluation is too expensive: as stated by Lin (2004), large scale manual evaluation of summaries as in the DUC conferences would require over 3000 hours of human efforts. Hence, an evaluation metric having high correlation with human scores would obviate the process of manual evaluation. In this section, we would look at some important recent papers that have been able to create standards in the summarization community.________________________________5.1 Human and Automatic Evaluation________________________________Lin and Hovy (2002) describe and compare various human and automatic metrics to evaluate summaries. They focus on the evaluation procedure used in the Document Understanding Conference 2001 (DUC-2001), where the Summary Evaluation Environment (SEE) interface was used to support the human evaluation part. NIST assessors in DUC-2001 compared manually written ideal summaries with summaries generated automatically by summarization systems and baseline summaries. Each text was decomposed into a list of units (sentences) and displayed in separate windows in SEE. To measure the content of summaries, assessors stepped through each model unit (MU) from the ideal summaries and marked all system units (SU) sharing content with the current model unit, rating them with scores in the range 1 − 4 to specify that the marked system units express all (4), most (3), some (2) or hardly any (1) of the content of the current model unit. Grammaticality, cohesion, and coherence were also rated similarly by the assessors. The weighted recall at threshold t (where t range from 1 to 4) is then deﬁned as________________________________Recallt = Number of MUs marked at or above t________________________________Number of MUs in the model summary. (13)________________________________An interesting study is presented that shows how unstable the human markings for overlapping units are. For multiple systems, the coverage scores assigned to the same units were diﬀerent by human assessors 18% of the time for the single document task and 7.6% of the time for multi-document task. The authors also observe that inter-human agreement is quite low in creating extracts from documents (∼ 40% for single-documents and ∼ 29% for multi-documents). To overcome the instability of human evaluations, they proposed using automatic metrics for summary evaluation.________________________________23________________________________Inspired by the machine translation evaluation metric BLEU (Papineni et al., 2001), they outline an accumulative n-gram matching score (which they call NAMS),________________________________NAMS = a1 · NAM1 + a2 · NAM2 + a3 · NAM3 + a4 · NAM4, (14)________________________________where the NAMn n-gram hit ratio is deﬁned as:________________________________# of matched n-grams between MU and S________________________________total # of n-grams in MU (15)________________________________with S denoting here the whole system summary, and where only content words were used in forming the n-grams. Diﬀerent conﬁgurations of ai were tried; the best correlation with human judgement (using Spearman’s rank order correlation coeﬃcient) was achieved using a conﬁguration giving 2/3 weight to bigram matches and 1/3 to unigrams matches with stemming done by the Porter stemmer.________________________________5.2 ROUGE________________________________Lin (2004) introduced a set of metrics called Recall-Oriented Understudy for Gisting Evaluation (ROUGE)23 that have become standards of automatic evaluation of summaries. In what follows, let R = {r1, . . . , rm} be a set of reference summaries, and let s be a summary generated automatically by some system. Let Φn(d) be a binary vector representing the n-grams contained in a document d; the i-th component φi n(d) is 1 if the i-th n-gram is contained in d and 0 otherwise. The metric ROUGE-N is an n-gram recall based statistic that can be computed as follows:________________________________ROUGE-N(s) = P Pr∈R⟨Φn(r), Φn(s)⟩ r∈R⟨Φn(r), Φn(r)⟩, (16)________________________________where ⟨., .⟩ denotes the usual inner product of vectors. This measure is closely related to BLEU which is a precision related measure. Unlike other measures previously considered, ROUGE-N can be used for multiple reference summaries, which is quite useful in practical situations. An alternative is taking the most similar summary in the reference set,________________________________ROUGE-Nmulti(s) = max r∈R ⟨Φn(r), Φn(s)⟩ ⟨Φn(r), Φn(r)⟩. (17)________________________________Another metric in (Lin, 2004) applies the concept of longest common subsequences24 (LCS). The rationale is: the longer the LCS between two summary sentences, the more similar they are. Let r1, . . . , ru be the reference sentences of the documents in R, and s a candidate summary (considered as a concatenation of sentences). The ROUGE-L is deﬁned as an LCS based F-measure:________________________________ROUGE-L(s) = (1 + β2)RLCSPLCS________________________________RLCS + β2PLCS (18)________________________________23See http://openrouge.com/default.aspx. 24A subsequence of a string s = s1 . . . sn is a string of the form si1 . . . sin where 1 ≤ i1 < . . . in ≤ n.________________________________24________________________________where RLCS(s) = Pu i=1 LCS(ri,s) Pu i=1 |ri| , PLCS(s) = Pu i=1 LCS(ri,s)________________________________|s| , |x| denotes the length of sentence x, LCS(x, y) denotes the length of the LCS between sentences x and y, and β is a (usually large) parameter to balance precision and recall. Notice that the LCS function may be computed by a simple dynamic programming approach. The metric (18) is further reﬁned by including weights that penalize subsequence matches that are not consecutive, yielding a new measure denoted ROUGE-W. Yet another measure introduced by Lin (2004) is ROUGE-S, which can be seen as a gappy version of ROUGE-N for n = 2 and is aptly called skip bigram. Let Ψ2(d) be a binary vector indexed by ordered pairs of words; the i-th component ψi 2(d) is 1 if the i-th pair is a subsequence of d and 0 otherwise. The metric ROUGE-S is computed as follows:________________________________ROUGE-S(s) = (1 + β2)RSPS________________________________RS + β2PS (19)________________________________where RS(s) = Pu i=1⟨Ψ2(ri),Ψ2(s)⟩ Pu i=1⟨Ψ2(ri),Ψ2(ri)⟩ and PS(s) = Pu i=1⟨Ψ2(ri),Ψ2(s)⟩ ⟨Ψ2(s),Ψ2(s)⟩ . The various versions of ROUGE were evaluated by computing the correlation coeﬃcient between ROUGE scores and human judgement scores. ROUGE-2 performed the best among the ROUGE-N variants. ROUGE-L, ROUGE-W, and ROUGE-S all performed very well on the DUC-2001 and DUC-2002 datasets. However, correlation achieved with human judgement for multi-document summarization was not as high as single-document ones; improvement on this side of the paradigm is an open research topic.________________________________5.3 Information-theoretic Evaluation of Summaries________________________________A very recent approach (Lin et al., 2006) proposes to use an information-theoretic method to automatic evaluation of summaries. The central idea is to use a divergence measure between a pair of probability distributions, in this case the JensenShannon divergence, where the ﬁrst distribution is derived from an automatic summary and the second from a set of reference summaries. This approach has the advantage of suiting both the single-document and the multi-document summarization scenarios. Let D = {d1, . . . , dn} be the set of documents to summarize (which is a singleton set in the case of single-document summarization). Assume that a distribution parameterized by θR generates reference summaries of the documents in D. The task of summarization can be seen as that of estimating θR. Analogously, assume that every summarization system is governed by some distribution parameterized by θA. Then, we may deﬁne a good summarizer as one for which θA is close to θR. One information-theoretic measure between distributions that is adequate for this is the KL divergence (Cover and Thomas, 1991),________________________________KL(pθA||pθR) =________________________________m X________________________________i=1 pθA i log pθA i________________________________pθR i . (20)________________________________However, the KL divergence is unbounded and goes to inﬁnity whenever pθA i vanishes________________________________25________________________________and pθR i does not, which requires using some kind of smoothing when estimating the distributions. Lin et al. (2006) claims that the measure used here should also be symmetric,25 another thing that the KL divergence is not. Hence, they propose to use the Jensen-Shannon divergence which is bounded and symmetric:26________________________________JS(pθA||pθR) = 1 2KL(pθA||r) + 1________________________________2KL(pθR||r) =________________________________= H(r) − 1________________________________2H(pθA) − 1________________________________2H(pθA), (21)________________________________where r = 1________________________________2pθA + 1________________________________2pθR is the average distribution. To evaluate a summary SA given a reference summary SR, the authors propose to use the negative JS divergence between the estimates of pθA and pθR given the summaries, score(SA|SR) = −JS(p ˆθA||p ˆθR) (22)________________________________The parameters are estimated via a posteriori maximization assuming a multinomial generation model for each summary (which means that they are modeled as bags-of-words) and using Dirichlet priors (the conjugate priors of the multinomial family). So: ˆθA = arg max θA p(SA|θA)p(θA), (23)________________________________where (m being the number of distinct words, a1, . . . , am being the word counts in the summary, a0 = Pm i=1 ai)________________________________p(SA|θA) = Γ(a0 + 1) Qm i=1 Γ(ai + 1)________________________________m Y________________________________i=1 θA,iai (24)________________________________and________________________________p(θA) = Γ(α0) Qm i=1 Γ(αi)________________________________m Y________________________________i=1 θA,iαi−1 (25)________________________________where αi are hyper-parameters and α0 = Pm i=1 αi. After some algebra, we get________________________________ˆθA,i = ai + αi − 1________________________________a0 + α0 − m (26)________________________________which is similar to MLE with smoothing.27 ˆθR is estimated analogously using the reference summary SR. Not surprisingly, if we have more than one reference summary, the MAP estimation given all summaries equals MAP estimation given their concatenation into a single summary.________________________________25However, the authors do not give much support for this claim. In our view, there is no reason to require symmetry. 26Although this is not mentioned in (Lin et al., 2006), the Jensen-Shannon divergence also satisﬁes the axioms to be a squared metric, as shown by Endres and Schindelin (2003). It has also a plethora of properties that are presented elsewhere, but this is out of scope of this survey. 27In particular if αi = 1 it is just maximum likelihood estimation (MLE).________________________________26________________________________The authors experimented three automatic evaluation schemes (JS with smoothing, JS without smoothing, and KL divergence) against manual evaluation; the best performance was achieved by JS without smoothing. This is not surprising since, as seen above, the JS divergence is bounded, unlike the KL divergence, and so it does not require smoothing. Smoothing has the eﬀect of pulling the two distributions more close to the uniform distribution.________________________________6 Conclusion________________________________The rate of information growth due to the World Wide Web has called for a need to develop eﬃcient and accurate summarization systems. Although research on summarization started about 50 years ago, there is still a long trail to walk in this ﬁeld. Over time, attention has drifted from summarizing scientiﬁc articles to news articles, electronic mail messages, advertisements, and blogs. Both abstractive and extractive approaches have been attempted, depending on the application at hand. Usually, abstractive summarization requires heavy machinery for language generation and is diﬃcult to replicate or extend to broader domains. In contrast, simple extraction of sentences have produced satisfactory results in large-scale applications, specially in multi-document summarization. The recent popularity of eﬀective newswire summarization systems conﬁrms this claim. This survey emphasizes extractive approaches to summarization using statistical methods. A distinction has been made between single document and multidocument summarization. Since a lot of interesting work is being done far from the mainstream research in this ﬁeld, we have chosen to include a brief discussion on some methods that we found relevant to future research, even if they focus only on small details related to a general summarization process and not on building an entire summarization system. Finally, some recent trends in automatic evaluation of summarization systems have been surveyed. The low inter-annotator agreement ﬁgures observed during manual evaluations suggest that the future of this research area heavily depends on the ability to ﬁnd eﬃcient ways of automatically evaluating these systems and on the development of measures that are objective enough to be commonly accepted by the research community.________________________________Acknowledgements________________________________We would like to thank Noah Smith and Einat Minkov for valuable suggestions during the course of the survey. We would also like to thank Alex Rudnicky and Mohit Kumar for insightful discussions at various points during 2006-2007.________________________________27________________________________References________________________________Amari, S.-I. and Nagaoka, H. (2001). Methods of Information Geometry (Translations of Mathematical Monographs). Oxford University Press. [20]________________________________Aone, C., Okurowski, M. E., Gorlinsky, J., and Larsen, B. (1999). A trainable summarizer with knowledge acquired from robust nlp techniques. In Mani, I. and Maybury, M. T., editors, Advances in Automatic Text Summarization, pages 71–80. MIT Press. [4, 5]________________________________Barzilay, R. and Elhadad, M. (1997). Using lexical chains for text summarization. In Proceedings ISTS’97. [8]________________________________Barzilay, R., McKeown, K., and Elhadad, M. (1999). Information fusion in the context of multi-document summarization. In Proceedings of ACL ’99. [12, 13, 14, 16]________________________________Baxendale, P. (1958). Machine-made index for technical literature an experiment. IBM Journal of Research Development, 2(4):354–361. [2, 3, 5]________________________________Brown, F., Pietra, V. J. D., Pietra, S. A. D., and Mercer, R. L. (1993). The mathematics of statistical machine translation: parameter estimation. Comput. Linguist., 19(2):263–311. [18]________________________________Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., and Hullender, G. (2005). Learning to rank using gradient descent. In ICML ’05: Proceedings of the 22nd international conference on Machine learning, pages 89– 96, New York, NY, USA. ACM. [8]________________________________Carbonell, J. and Goldstein, J. (1998). The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of SIGIR ’98, pages 335–336, New York, NY, USA. [12, 14, 15]________________________________Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. PhD thesis, University of Pennsylvania. [13, 20]________________________________Conroy, J. M. and O’leary, D. P. (2001). Text summarization via hidden markov models. In Proceedings of SIGIR ’01, pages 406–407, New York, NY, USA. [6]________________________________Cover, T. and Thomas, J. (1991). Elements of Information Theory. Wiley. [25]________________________________Daumé III, H. and Marcu, D. (2002). A noisy-channel model for document compression. In Proceedings of the Conference of the Association of Computational Linguistics (ACL 2002). [20]________________________________Daumé III, H. and Marcu, D. (2004). A tree-position kernel for document compression. In Proceedings of DUC2004. [20]________________________________Edmundson, H. P. (1969). New methods in automatic extracting. Journal of the ACM, 16(2):264–285. [2, 3, 4]________________________________28________________________________Endres, D. M. and Schindelin, J. E. (2003). A new metric for probability distributions. IEEE Transactions on Information Theory, 49(7):1858–1860. [26]________________________________Evans, D. K. (2005). Similarity-based multilingual multi-document summarization. Technical Report CUCS-014-05, Columbia University. [12, 17]________________________________Gous, A. (1999). Spherical subfamily models. [20, 21]________________________________Hall, K. and Hofmann, T. (2000). Learning curved multinomial subfamilies for natural language processing and information retrieval. In Proc. 17th International Conf. on Machine Learning, pages 351–358. Morgan Kaufmann, San Francisco, CA. [20]________________________________Hovy, E. and Lin, C. Y. (1999). Automated text summarization in summarist. In Mani, I. and Maybury, M. T., editors, Advances in Automatic Text Summarization, pages 81–94. MIT Press. [17]________________________________Knight, K. and Marcu, D. (2000). Statistics-based summarization step one: Sentence compression. In AAAI/IAAI, pages 703–710. [19]________________________________Kupiec, J., Pedersen, J., and Chen, F. (1995). A trainable document summarizer. In Proceedings SIGIR ’95, pages 68–73, New York, NY, USA. [4]________________________________Lebanon, G. (2006). Sequential document representations and simplicial curves. In Proceedings of the 22nd Conference on Uncertainty in Artiﬁcial Intelligence. [22]________________________________Lebanon, G., Mao, Y., and Dillon, J. (2007). The locally weighted bag of words framework for document representation. J. Mach. Learn. Res., 8:2405–2441. [21, 22]________________________________Lin, C.-Y. (1999). Training a selection function for extraction. In Proceedings of CIKM ’99, pages 55–62, New York, NY, USA. [5]________________________________Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In Marie-Francine Moens, S. S., editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain. [8, 23, 24, 25]________________________________Lin, C.-Y., Cao, G., Gao, J., and Nie, J.-Y. (2006). An information-theoretic approach to automatic evaluation of summaries. In Proceedings of HLT-NAACL ’06, pages 463–470, Morristown, NJ, USA. [25, 26]________________________________Lin, C.-Y. and Hovy, E. (1997). Identifying topics by position. In Proceedings of the Fifth conference on Applied natural language processing, pages 283–290, San Francisco, CA, USA. [5]________________________________Lin, C.-Y. and Hovy, E. (2002). Manual and automatic evaluation of summaries. In Proceedings of the ACL-02 Workshop on Automatic Summarization, pages 45–51, Morristown, NJ, USA. [23]________________________________29________________________________Luhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal of Research Development, 2(2):159–165. [2, 3, 6, 8]________________________________Mani, I. and Bloedorn, E. (1997). Multi-document summarization by graph search and matching. In AAAI/IAAI, pages 622–628. [15, 16]________________________________Marcu, D. (1998a). Improving summarization through rhetorical parsing tuning. In Proceedings of The Sixth Workshop on Very Large Corpora, pages 206-215, pages 206–215, Montreal, Canada. [9, 10, 20]________________________________Marcu, D. C. (1998b). The rhetorical parsing, summarization, and generation of natural language texts. PhD thesis, University of Toronto. Adviser-Graeme Hirst. [10]________________________________McKeown, K., Klavans, J., Hatzivassiloglou, V., Barzilay, R., and Eskin, E. (1999). Towards multidocument summarization by reformulation: Progress and prospects. In AAAI/IAAI, pages 453–460. [11, 12, 13, 14, 16]________________________________McKeown, K. R. and Radev, D. R. (1995). Generating summaries of multiple news articles. In Proceedings of SIGIR ’95, pages 74–82, Seattle, Washington. [8, 11, 12]________________________________Miller, G. A. (1995). Wordnet: a lexical database for english. Commun. ACM, 38(11):39–41. [4, 9]________________________________Nenkova, A. (2005). Automatic text summarization of newswire: Lessons learned from the document understanding conference. In Proceedings of AAAI 2005, Pittsburgh, USA. [7]________________________________Ono, K., Sumita, K., and Miike, S. (1994). Abstract generation based on rhetorical structure extraction. In Proceedings of Coling ’94, pages 344–348, Morristown, NJ, USA. [9]________________________________Osborne, M. (2002). Using maximum entropy for sentence extraction. In Proceedings of the ACL’02 Workshop on Automatic Summarization, pages 1–8, Morristown, NJ, USA. [7]________________________________Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001). Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL ’02, pages 311–318, Morristown, NJ, USA. [24]________________________________Radev, D. R., Hovy, E., and McKeown, K. (2002). Introduction to the special issue on summarization. Computational Linguistics., 28(4):399–408. [1, 2]________________________________Radev, D. R., Jing, H., and Budzikowska, M. (2000). Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies. In NAACL-ANLP 2000 Workshop on Automatic summarization, pages 21–30, Morristown, NJ, USA. [12, 16, 17]________________________________30________________________________Radev, D. R., Jing, H., Stys, M., and Tam, D. (2004). Centroid-based summarization of multiple documents. Information Processing and Management 40 (2004), 40:919–938. [16, 17]________________________________Radev, D. R. and McKeown, K. (1998). Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24(3):469–500. [12]________________________________Salton, G. and Buckley, C. (1988). On the use of spreading activation methods in automatic information. In Proceedings of SIGIR ’88, pages 147–160, New York, NY, USA. [15]________________________________Salton, G., Wong, A., and Yang, A. C. S. (1975). A vector space model for automatic indexing. Communications of the ACM, 18:229–237. [20]________________________________Selman, B., Levesque, H. J., and Mitchell, D. G. (1992). A new method for solving hard satisﬁability problems. In AAAI, pages 440–446. [11]________________________________Svore, K., Vanderwende, L., and Burges, C. (2007). Enhancing single-document summarization by combining RankNet and third-party sources. In Proceedings of the EMNLP-CoNLL, pages 448–457. [7, 8]________________________________Witbrock, M. J. and Mittal, V. O. (1999). Ultra-summarization (poster abstract): a statistical approach to generating highly condensed non-extractive summaries. In Proceedings of SIGIR ’99, pages 315–316, New York, NY, USA. [18]________________________________31