HAL Id: hal-01297034 https://hal.science/hal-01297034 Preprint submitted on 1 Apr 2016 HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L‚Äôarchive ouverte pluridisciplinaire HAL, est destin√©e au d√©p√¥t et √† la diffusion de documents scientifiques de niveau recherche, publi√©s ou non, √©manant des √©tablissements d‚Äôenseignement et de recherche fran√ßais ou √©trangers, des laboratoires publics ou priv√©s. New approach for Bandwidth Selection in the Kernel Density Estimation Based on Œ≤-Divergence Hamza Dhaker, Papa Ngom, Pierre Mendy, El Hadji Deme To cite this version: Hamza Dhaker, Papa Ngom, Pierre Mendy, El Hadji Deme. New approach for Bandwidth Selection in the Kernel Density Estimation Based on Œ≤-Divergence. 2016. Ôøøhal-01297034Ôøø New approach for Bandwidth Selection in the Kernel Density Estimation Based on Œ≤-Divergence. Hamza Dhakera,b, Papa Ngoma,‚àó, Pierre Mendyb, El Hadji Demec aLMA,Laboratoire de Mathematiques Appliquees bLMDAN,Universit Cheikh Anta Diop, Dakar-Fann, BP 5005, Senegal cLERSTAD, Universite Gaston Berger, UFR SAT Saint-Louis, BP 234, Senegal Abstract The choice of bandwidth is crucial to the kernel density estimation KDE. Various bandwidth selection methods for KDE least squares cross-validation LSCV and Kullback-Leibler cross-validation are proposed. We propose a method to select the optimal bandwidth for the KDE. The idea behind this method is to generalize the LSCV method, using the measure of Œ≤-divergence, and to see the importance of improving our method, we will compares these DŒ≤(bfh, f) bandwidth selector with a normal reference(NR), the last squares cross-validation(LSCV), the Sheather and Jones (S J) method, and the generalized LSCV(LSCVg) bandwidth selector, on simulated data. The use of the various practical bandwidth selectors is illustrated on a real data example. Keywords: nonparametric density estimation, Œ≤-divergence, integrated squared error, bandwidth AMS Subject ClassiÔ¨Åcation : 62G07, 94A17 . 1. Introduction 1 The problem of choosing the bandwidth (window width or smoothing parameter) h is importantly in statistical es2 timation of the kernel density estimation. A vast amount of literatures has been devoted in choosing practical optimal 3 bandwidth for techniques built on kernel estimation et some comparative studies have been made to these methods. 4 Representative surveys of bandwidth selection techniques can be found in Bowman[3], Jones et al.[9], Loader[12], 5 Peter Hall [5] Scott[18],and Wand and Jones[24]. 6 Least-squares cross-validation (LSCV) is Among the earliest bandwidth selectors, this method was suggested by 7 Rudemo [17] and Bowman [2], in the 80s ,it has been the method of reference, but in the early 90s, studies have 8 shown that other methods performs better from the bias points of view and much better in reducing the variance. See 9 Park and Turlach [15] for a detailed description. 10 Interesting comparative studies have been published. Bowman[2] compared two methods for selecting bandwidth, 11 The Ô¨Årst is the Kullback-Leibler Cross-Validation and the second is that of Integrated Squared Error Cross-Validation. 12 Scott and Terrell [19] compared the two methods by theoretical calculation of the noise in the cross-validation func13 tion and corresponding cross-validated smoothing parameters, by Monte Carlo simulation, and by example. 14 Sheather and Jones [20] set up a plug-in type of three-step procedure. They choose to estimate R( f ‚Ä≤‚Ä≤) (the term un15 known in AMIS E). 16 Jin Zhang [26]) have proposed a generalization the classical least squares cross-validation (LSCV) selector for its 17 variability and under smoothing, He did a comparison of bandwidths for Ô¨Ånite sample behavior. 18 For a more complete treatment, from a historical viewpoint, with complete references, and detailed discussion of 19 variations that have been suggested, see Jones et al. [10] Quick access to implementation of most of the methods 20 discussed here has been provided by park and Jones et al. [9] . 21 22 ‚àóCorresponding author Email address: papa.ngom@ucad.edu.sn (Papa Ngom) Preprint submitted to Journal Name April 1, 2016 The main purpose of this paper is to investigate the optimal bandwidth hŒ≤ for minimizing criterion DŒ≤(bfn, f) error. 23 We will see that DŒ≤(bfn, f) generalize the Integrated Square Error (IS E) and Kullback-Leiber divergence (KL). 24 After having introduced the DŒ≤(bfn, f) selection method we study the Ô¨Ånite sample performances of various bandwidth 25 selectors via a simulation study. We compare Ô¨Åve procedures: the normal reference (NR) method, the last squares 26 cross-validation (LSCV), the Sheather and Jones (S J) method, the generalized LSCV(LSCVg) and criterion DŒ≤(bfn, f) 27 error. 28 This paper is organized as follows. Section 2 describes the classical methods for bandwidth selections. Section 3 29 presents the new method proposed for bandwidth selector, which generalizes and provides improved for the least 30 squares cross-validation (LSCV). In Section 4 we present some simulation results for estimation and comparison of 31 the various methods. Section 5 applies the methods to real data. Finally, the conclusion and perspective is presented 32 in Section 6. 33 2. Classical Methods for Bandwidth Selection 34 Given an n-sample X1, X2, ..., Xn of independent random variables and same unknown density f. Consider the 35 Parzen-Rosenblatt kernel estimator of the density f given by: 36 bfh(x) = 1 nh n X i=1 K  x ‚àí Xi h  (1) h > 0 where is the bandwidth and K. a density function deÔ¨Åned on R called kernel. 37 To estimate f, choose the kernel K and h parameter. If the choice of the kernel is not a problem, it is not the case for 38 the choice of the width of the window h which essentially depends on the size n of the sample. There are two methods 39 of families: the family of cross-validation methods and the family of plug-in methods. 40 The decision of an optimal choice for the bandwidth suppose the speciÔ¨Åcation of an error criterion that can be opti41 mized. The criterion is to minimize the Mean Integrated Square Error (MIS E). In this case, [E. Parzen [16].] there is 42 obtained 43 MIS E(bfh(x)) = Z E hbfh(x) ‚àí f(x) i2 dx = h4 4 ¬µ2(K)2 Z  f ‚Ä≤‚Ä≤(x) 2 dx + R(K) nh + O   h5 + 1 n ! (2) where ¬µ2(K) = R x2K(x)dx is the variance of kernel K and R(g) = R g2(x)dx for any function g. 44 The Asymptotics Mean Integrated Square Error (AMISE) is then of the form: 45 AMIS E(bfh) = h4 4 ¬µ2(K)2R( f ‚Ä≤‚Ä≤) + R(K) nh (3) To Ô¨Ånd the closed form expression for hAMIS E, begin by diÔ¨Äerentiating (3) to obtain ‚àÇAMIS E ‚àÇh = ‚àí(nh2)‚àí1R(K) + h3¬µ2(K)2R( f ‚Ä≤‚Ä≤) Setting this equation equal to 0 and solving for h produces 46 hAMIS E = " R(K) n¬µ2(K)2R( f ‚Ä≤‚Ä≤) #1/5 (4) It is found that the optimal width of hAMIS E window depends on the unknown density f through the parameter 47 R( f ‚Ä≤‚Ä≤), which has to be estimated before using hAMIS E. 48 49 2 A very natural way to get around the problem of not knowing f ‚Ä≤‚Ä≤ is to use a standard family of distributions to 50 assign a value of the term R( f ‚Ä≤‚Ä≤) in expression (4). For example, assume that a density f belongs to the Gaussian family 51 with mean ¬µ and variance œÉ, then 52 R( f ‚Ä≤‚Ä≤) = Z  f ‚Ä≤‚Ä≤(x) 2 dx = œÉ‚àí5 R  œÜ ‚Ä≤‚Ä≤(x) 2 dx = 3 8œÄ‚àí12œÉ‚àí5 ‚âà 0.212œÉ‚àí5 (5) where œÜ(x) is the standard normal density. If one uses a Gaussian kernel, then 53 hNR = (4œÄ)‚àí1/10  3 8œÄ‚àí1/2‚àí1/5 œÉn‚àí1/5 =  4 3 1/5 œÉn‚àí1/5 (6) If we want to make this estimate more insensitive to outliers, we have to use a more robust estimate for the scale 54 parameter of the distribution. Let bR be the sample interquartile, then one possible choice for h is 55 hNR = 1.06min  bœÉ, bR (Œ¶(3/4)‚àíŒ¶(1/4))  n‚àí1/5 = 1.06min  bœÉ, bR 1.349  n‚àí1/5 (7) where Œ¶ is the standard normal distribution function. To see more detail (e.g., Silverman, [21]; H¬®ardle[8]; Scott, 56 1992). 57 The LSCV, sometimes called an unbiased cross-validation was proposed by Rudemo [17] and Bowman [2]. The 58 criterion is to choose the bandwidth that minimizes an estimator of Integrated Square Error (IS E): 59 IS E = Z bf 2 h (x)dx ‚àí 2 Z bfh(x) f(x)dx + Z f 2(x)dx (8) The ideal choice of bandwidth is the one which minimizes: 60 L(h) = IS E ‚àí Z f 2(x)dx = Z bf 2 h (x)dx ‚àí 2 Z bfh(x) f(x)dx (9) The principle of the least squares cross-validation method is to Ô¨Ånd an estimate of L(h) from the data and minimize 61 it over h. Consider the estimator 62 LSCV(h) = Z bf 2 h (x)dx ‚àí 2 n n X i=1 bfh(i)(Xi) (10) with Z bf 2 h (x)dx = 1 n2h n X i=1 n X j=1 (k ‚àó k)  Xi ‚àí Xj h ! and 63 bfh(i)(Xi) = 1 h(n ‚àí 1) n X j,i K  Xi ‚àí Xj h ! (11) where ‚àó represents the convolution. 64 Further discussion on this method can be found in Bowman [3], and Hall and Marron [7]. Under mild conditions, Hall 65 [5] and Stone [22] proved that hLSCV is asymptotically the best in the sense of minimizing MIS E(bfh). 66 67 3 Sheather and Jones (1991) introduced a reliable bandwidth selector bhS J, which is a plug-in estimator of hAMIS E, 68 the idea of Sheather and Jones is to estimate the quantity R( f ‚Ä≤‚Ä≤) by an estimator of E  f (4)(X)  , by remarking that 69 R( f ‚Ä≤‚Ä≤) = E  f (4)(X)  = R f (4)(x) f(x)dx. 70 71 Jin Zhang [26] introduced a generalization of classical least squares cross-validation(LSCV), his method provides 72 a signiÔ¨Åcant improvement for (LSCV). 73 Used it as the case that K is the Gaussian kernel œÜ. According to Equation 10, 74 LSCV(h) = œÜ ‚àö 2h(0) n ‚àí 2 n(n ‚àí 1) X i<j " 2œÜh(Xi ‚àí Xj) +  1 n ‚àí 1 ! œÜ ‚àö 2h(Xi ‚àí Xj) # (12) Jin Zhang [26] is generalized LSCV by: 75 LSCVg(h) = œÜ ‚àö 2h(0) n + 2 n(n ‚àí 1) X i< j " 2 g(g ‚àí 2)œÜ ‚àögh(Xi ‚àí Xj) +  1 n ‚àí 1 g ‚àí 2 ! œÜ ‚àö 2h(Xi ‚àí X j) # (13) g with a positive number. 76 The generalized LSCV bandwidth selector hLSCVg is deÔ¨Åned as the minimize of LSCVg(h) over h 77 3. Œ≤-Divergence for Bandwidth Selection 78 The basic Beta-divergence was introduced by Basu et al. [1] and Minami and Eguchi [14]. 79 The Œ≤-Divergence measure for bandwidth selection will be introduced in this section to improve the behavior of 80 the choice for bandwidth. 81 DŒ≤(bfh, f(x)) = 1 Œ≤ Z S bf Œ≤ h (x)dx ‚àí 1 Œ≤ ‚àí 1 Z S bf Œ≤‚àí1 h (x) f(x)dx + 1 Œ≤(Œ≤ ‚àí 1) Z S f Œ≤(x)dx in the case Œ≤ = 2, 82 2D2(bfh, f(x)) = IS E(bfh) = Z S (bfh(x) ‚àí f(x))2dx So we can say that IS E is a special case of DŒ≤. Note, the optimal bandwidth that minimizes DŒ≤(bfn,h, f(x)) is equivalent to the bandwidth that minimizes the expected value of the quantity: DŒ≤(h) = DŒ≤(bfh, f(x)) ‚àí 1 Œ≤(Œ≤ ‚àí 1) Z S f Œ≤(x)dx = 1 Œ≤ Z S bf Œ≤ h (x)dx ‚àí 1 Œ≤ ‚àí 1 Z S bf Œ≤‚àí1 h (x) f(x)dx The principle of the least squares cross-validation method is to Ô¨Ånd an estimate of DŒ≤(h) from the data and minimize 83 it over h. Consider the estimator, 84 DŒ≤CV(h) = 1 Œ≤ Z S bf Œ≤ h (x)dx ‚àí 1 n(Œ≤ ‚àí 1) n X i=1 bf Œ≤‚àí1 h(i) (Xi) (14) with bf Œ≤‚àí1 h(i) (Xi) is deÔ¨Åned in (11) 85 When we want to implement this technique on the computer, the computation of minimized DŒ≤CV(h) for a of band86 widths h may be based on the following algorithm: 87 88 4 Algorithm 1 algorithm for minimize DŒ≤CV(h) 1: for i = 1 to n do 2: c1 = 1 nhŒ≤‚àí1 3: c2 = 1 Œ≤hnŒ≤‚àí1 4: c3 = 1 (Œ≤‚àí1)(n‚àí1)Œ≤‚àí1 5: S um = 0 6: S um1 = 0 7: for j = 1 to n do 8: S um1 = S um1 + K  Xi‚àíXj h  9: S um = c2 ‚àó (S um1)Œ≤ ‚àí c3 ‚àó (S um1 ‚àí K (0))Œ≤‚àí1 10: end forDŒ≤CV(h) = c1 ‚àó S um 11: end for hDŒ≤CV = arg min h  DŒ≤CV(h)  Theorem 1. Let the following conditions on f be satisÔ¨Åed: 89 (F1) f is compactly supported on I. 90 (F2) f is four times continuously diÔ¨Äerentiable on I. 91 (F3) lim x‚àí‚Üí+ inf I f (i)(x) = lim x‚àí‚Üí‚àí sup I f (i)(x) , 1 ‚â§ j ‚â§ 3. 92 (F4) R I f (2)(x)2 f(x)Œ≤‚àí2dx < ‚àû. 93 As n ‚àí‚Üí ‚àû, the window width hEDŒ≤ that minimizes the mean Œ≤-divergence between a kernel estimator bfh and density 94 f satisÔ¨Åes 95 hŒ≤ = hEDŒ≤ = Ô£±Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£≥ R K(t)2dt R I f(x)Œ≤‚àí1dx hR t2K(t)dt i2 R I f(x)Œ≤‚àí2 f (2)(x)2dx Ô£ºÔ£¥Ô£¥Ô£¥Ô£ΩÔ£¥Ô£¥Ô£¥Ô£æ 1/5 n‚àí1/5 (15) in the particular case 96 ‚Ä¢ Œ≤ = 2 this case the Mean, integrated square error h2 = hMIS E(bfh) = Ô£±Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£≥ R K(t)2dt hR t2K(t)dt i2 R I f (2)(x)2dx Ô£ºÔ£¥Ô£¥Ô£¥Ô£ΩÔ£¥Ô£¥Ô£¥Ô£æ 1/5 n‚àí1/5. ‚Ä¢ Œ≤ = 1 this case the Kullback-Libler, h1 = hE(KL) = Ô£±Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£≥ R K(t)2dt R I dx hR t2K(t)dt i2 R I f(x)‚àí1 f (2)(x)2dx Ô£ºÔ£¥Ô£¥Ô£¥Ô£ΩÔ£¥Ô£¥Ô£¥Ô£æ 1/5 n‚àí1/5 Theorem1 is derived from the following proposition by assuming (F4) and by balancing the Ô¨Årst two terms in (16). 97 Proposition 1. Under (F1) ‚àí (F3) we have 98 EDŒ≤(bfh, f) = h4 8 (Z I t2K(t)dt )2 Z f(x)Œ≤‚àí2 f (2)(x)2dx + 1 2nh Z I (K(t))2 dt Z f(x)Œ≤‚àí1dx + O(n‚àí1 + h6) (16) 5 Choosing Œ≤: the Œ≤ value that minimizes equation (16) 99 ‚àÇEDŒ≤(bfh, f) ‚àÇŒ≤ = (Œ≤ ‚àí 2)h4 8 (Z I t2K(t)dt )2 Z f(x)Œ≤‚àí3 f (2)(x)2dx + Œ≤ ‚àí 1 2nh Z I (K(t))2 dt Z f(x)Œ≤‚àí2dx + O(n‚àí1 + h6) we pose ‚àÜ1 = h4 8 nR I t2K(t)dt o2 R f(x)Œ≤‚àí3 f (2)(x)2dx and ‚àÜ2 = 1 2nh R I (K(t))2 dt R f(x)Œ≤‚àí2dx 100 ‚àÇEDŒ≤(bfh, f) ‚àÇŒ≤ = (Œ≤ ‚àí 2)‚àÜ1 + (Œ≤ ‚àí 1)‚àÜ2 = 0 101 Œ≤ = 1 + ‚àÜ1 ‚àÜ1 + ‚àÜ2 we know that ‚àÜ1 ‚â• 0 and ‚àÜ2 ‚â• 0 This implies that: 1 < Œ≤ < 2 4. Proof 102 Proof. Proposition 1 103 With a random variable Œæ = Op(1) whose expectation is 0 and variance 1, we can write bfh(x) as (see [11]) 104 bfh(x) = f(x) Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞1 + h2 2 f (2)(x) f(x) Z I t2K(t)dt + h4 24 f (4)(x) f(x) Z I t4K(t)dt + O(h6) + Ô£±Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£≥ R I K(t)2dt nhf(x) Ô£ºÔ£¥Ô£¥Ô£ΩÔ£¥Ô£¥Ô£æ 1/2 Œæ + Op(n‚àí1/2) Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª , (17) Where the O(h6) terms depend upon x. Using (1 + z)Œ≤ = 1 + Œ≤z + Œ≤(Œ≤‚àí1) 2 z2 + O(z3) 105 bf Œ≤ h (x) = f Œ≤(x)[1 + Œ≤   1 2h2 f (2)(x) f(x) R I t2K(t)dt + 1 24h4 f (4)(x) f(x) R I t4K(t)dt +  R I K(t)2dt nh f(x) 1/2 Œæ ! + Œ≤(Œ≤‚àí1) 2  1 4h4 f (2)(x)2 f(x)2 R I t2K(t)dt 2 + R I K(t)2dt nh f(x) Œæ2  + O(h6) + Op(n‚àí1/2)] (18) bf Œ≤‚àí1 h (x) = f Œ≤‚àí1(x)[1 + (Œ≤ ‚àí 1)   1 2h2 f (2)(x) f(x) R I t2K(t)dt + 1 24h4 f (4)(x) f(x) R I t4K(t)dt +  R I K(t)2dt nh f(x) 1/2 Œæ ! + (Œ≤‚àí1)(Œ≤‚àí2) 2  1 4h4 f (2)(x)2 f(x)2 R I t2K(t)dt 2 + R I K(t)2dt nh f(x) Œæ2  + O(h6) + Op(n‚àí1/2)] (19) DŒ≤(bfh, f) = R f(x)Œ≤  1 Œ≤ ‚àí 1 Œ≤‚àí1 +  Œ≤‚àí1 2 ‚àí Œ≤‚àí2 2   1 4h4 f (2)(x)2 f(x)2 R I t2K(t)dt 2 + R I K(t)2dt nh f(x) Œæ2  + O(h2) + Op(n‚àí1/2)  dx + 1 Œ≤(Œ≤‚àí1) R f(x)bdx (20) = 1 2 R f(x)Œ≤  1 4h4 f (2)(x)2 f(x)2 R I t2K(t)dt 2 + R I K(t)2dt nh f(x) Œæ2 + O(h2) + Op(n‚àí1/2)  dx (21) EDŒ≤(bfh, f) = 1 2E R f(x)Œ≤  1 4h4 f (2)(x)2 f(x)2 R I t2K(t)dt 2 + R I K(t)2dt nh f(x) Œæ2 + O(h2) + Op(n‚àí1/2)  dx = h4 8 nR I t2K(t)dt o2 R f(x)Œ≤‚àí2 f (2)(x)2dx + 1 2nh R I (K(t))2 dt R f(x)Œ≤‚àí1E(Œæ2)dx + O(n‚àí1 + h6) we know that E(Œæ2) = 1 106 EDŒ≤(bfh, f) = h4 8 (Z I t2K(t)dt )2 Z f(x)Œ≤‚àí2 f (2)(x)2dx + 1 2nh Z I (K(t))2 dt Z f(x)Œ≤‚àí1dx + O(n‚àí1 + h6) (22) as required. 107 6 5. Simulation 108 We approximate the true density f by a normal mixture. 109 f(x) = J X j=1 œâjœÜœÉ j(x ‚àí ¬µj) (23) where J is a positive integer. œâ1, ..., œâJ is a set of positive numbers that sum to one, and for each j, ‚àí‚àû < ¬µj < ‚àû and 110 œÉ j > 0. The family of normal mixture densities used by (Marron and Wand [13]) is extremely rich, and, in fact, any 111 density can be approximated arbitrarily well by a member of this family. 112 for f given by Equation 23, the MIS E in Equation (2 ) of the kernel density estimator in Equation (1) have explicit 113 forms. In fact, 114 MIS E(bfh(x)) = (2œÄ1/2nh)‚àí1 + wT{(1 ‚àí n‚àí1)‚Ñ¶2 ‚àí 2‚Ñ¶1 + ‚Ñ¶0}w (24) (Marron and Wand [13]) where w = (œâ1, ..., œâJ) and ‚Ñ¶a is the J √ó J matrix having (j √ó j) entry equal to œÜ q ah2+œÉ2 j+œÉ2 j‚Ä≤ (¬µ j ‚àí ¬µj‚Ä≤) . 115 We consider the normal mixture in the case of J = 2 and œâ1 = œâ2 = 0.5, similar similation were performed by Jin 116 Zhang [26]. Therefore the true density is: 117 f(x) = 0.5œÜ(x) + 0.5œÜœÉ(x ‚àí ¬µ) (25) Based on 50, 200, 700 draws from f in the case where ¬µ = 0, 1, 5 and œÉ = 1, 0.5, 0.1. Table 1 give the exhibits 118 the simulated relative eÔ¨Éciency RE bh  = MIS E bfbhMIS E  /MIS E bfbh  of the kernel estimator, using bandwidths bhNR, 119 bhLSCV, bhS J, bhLSCV4 and bhDŒ≤CV(with Œ≤ = 1.1, 1.5, 4 and 4), it is lower wherever than 1, because the optimal bandwidth 120 hMIS E minimize MIS E. Each bandwidth, mean E bh  and mean relation error E|bh/hMIS E ‚àí 1| are obtained, these 121 values are given by respectively Tables 2 and 3. 122 123 1. It can be seen that when the density f is not very far from normal, such as that cases of (¬µ, œÉ) = (0, 1), (0, 0.5), (1, 1) 124 and (1, 0.5), bandwidth which are obtained by using NR criterion performs well. In other cases it usually has 125 the smallest RE(bh) and largest E(bh),tending to over smooth its kernel density estimate the most. 126 2. From these tables it can be seen that bandwidth bhLSCV which are obtained by using LSCV performs poorly if 127 the true density is close to normal usually having the smallest RE(bh) and E(bh), then the kernel density estimator 128 is undersmoothed. But in the contrary case, bhLSCV can perform very well, and in many situations, in Table 2 it 129 is seen that E(bhLSCV) is close to the optimal bhMIS E, but the corresponding E(bhLSCV/bhMIS E ‚àí 1) is large, which 130 means that the bias ofbhLSCV is small but its variation is large. 131 3. The plug-in bandwidth bhS J seen to be the best existing bandwidth selectors (as commented by Venables and 132 Ripley [23]). bhS J among the best bandwidth selectors in most situations. However it behaves very poorly for 133 small œÉ (the density curve is sharp), oversmoothing its kernel density curve by overestimatingbhMIS E. 134 4. As commented by J. Zhang [26], The generalized LSCV bandwidth bhLSCV4 seems to be bandwidth selectors 135 that is always among the best for having large RE(bh) and small E|bh/hMIS E ‚àí 1|. 136 5. The proposed bandwidth selector method (bhDŒ≤CV) completely dominate the selection methods Bandwidth as n 137 increases, otherwise bhDŒ≤CV (with Œ≤ = 1.1, 1.5, 1.8 and 4) are with bhLSCV4 bandwidths selectors that are always 138 among the best. for having large RE(bhDŒ≤CV) and small E|bhDŒ≤CV/hMIS E ‚àí 1| for most cases. They signiÔ¨Åcantly 139 improve the classicalbhLSCV. Indeed Figue1 shows that the increase in n causes a value of RE(bhDŒ≤CV) close to 1 140 7 and greater than RE(bhLSCV) 141 Figure 2 show that for the DŒ≤CV, this bias does not have a serious eÔ¨Äect on the eÔ¨Éciency of the method, since 142 the RE(bhDŒ≤CV) is relatively close to 1. We can conclude from Figure 2 our practical selection procedures DŒ≤CV 143 have a performance close to the one for the LSCV4 bandwidth selector (since the ratios are relatively close to 144 1). 145 8 Figure 1: RE(bh) using bandwidthsbhLSCV,bhLSCV4 andbhDŒ≤CV with Œ≤ = 1.1, 1.5, 1.8, 4 9 Figure 2: Kernel density estimates with diÔ¨Äerent bandwidths. NR, S J, LSCV, LSCV4, D1.1CV 6. Examples 146 In this Section, we will provided two examples to evaluate performance of our method compared to several clas147 sical bandwidth selection methods for Gaussian kernel density. The two data sets in the examples have been analyzed 148 by many authors ( Silverman [21] and J. Zhang [26]) to illustrate various kinds of methods in density estimation. 149 150 The Ô¨Årst example comprises the lengths of 86 spells of psychiatric treatment undergone by control patients used 151 as controls in a study of suicide risks reported by Copas and Fryer [4]; Silverman [21]. 152 Figure 2a plot the data points and the kernel density estimates for the suicide study data, when we using commonly 153 used bandwidthsbhNR = 35.78,bhS J = 23.16,bhLSCV = 15.69,bhLSCV4 = 22.57 and hD1.1CV = 24.25. 154 It seems that the density for the length of treatment is a unimodal curve heavily skewed to the right. it seems clear 155 than the bandwidth hNR oversmooth her kernel density curve and underestimate the peak near 20. 156 The bandwidths were obtained by using the methods of S J, LSCV4 and D1.1CV, are better because bhS J, bhLSCV4 and 157 bhD1.1CV well balance the two situations and seem to capture the true shape of the data. In this example,bhLSCV4 = 27.57 158 andbhD1.1CV = 24.25 are closer tobhS J = 23.16. 159 160 The second comprises the lengths in minutes of 107 eruption lengths in minutes for the Old Faithful geyser in 161 Yellowstone National Park, USA (source: Weisberg [25]; Silverman [21]). 162 Figure 2b plot the data points and the kernel density estimates for Old Faithful geyser data, we using bandwidths 163 bhNR = 0.4331,bhS J = 0.2250,bhLSCV = 0.1030,bhLSCV4 = 0.1740 andbhD1.1CV = 0.1480. 164 An important point to note that the density curve for eruption length is similar to bimodal normal density (normal 165 mixture). As commented by Zing, the bandwidthS bhNR is heavily over smoothes its kernel density curve, underesti166 mating the two peaks of the curve but overestimating the valley between them. On the other hand, bhLSCV seems to 167 undersmooth the curve too much, overestimating the two peaks but underestimating for the valley. 168 However bhS J, bhLSCV4 and bhD1.1CV, especially the later, are proper bandwidths for their density estimates to be able to 169 capture the feature of the true density curve. 170 10 7. Conclusions 171 The kernel estimator is the most used in density estimation. The main issue is bandwidth selection, which is a hot 172 topic and is still frustrating statisticians. A various bandwidth selection strategies have been proposed such as normal 173 reference bhNR , unbiased cross-validation bhLSCV, cross validation, Sheather-Jones method bhS J and and most recently 174 Zing proposed a bandwidthbhLSCVg. 175 The normal reference bandwidth bhNR method is limited the practical use, since they are restricted to situations where 176 a pre-speciÔ¨Åed family of densities is correctly selected. The popularity of the LSCV method is due to the intuitive 177 motivation and the fact that bhLSCV is asymptotically optimal under low conditions. Bandwidth bhS J seems to be the 178 best existing bandwidth selector. Unfortunately, it also tends to oversmooth the density estimate when the true density 179 curve is sharp. 180 We have attempted to evaluate choice the optimal bandwidth bhLSCV, using Œ≤-divergence. Compared to traditional 181 bandwidth selection methods designed for kernel density estimation, our proposed DŒ≤CV bandwidth selection method 182 is always one of the best for having large RE bh  and small E|bh/hMIS E‚àí1|. Simulation studies showed that our proposed 183 novel optimal bandwidth method designed for kernel density estimation signiÔ¨Åcantly improves the classicalbhLSCV for 184 its variability and undersmoothing, adapts to diÔ¨Äerent situations, and outperforms other bandwidths. 185 References 186 [1] Basu, A.; Harris, I.R.; Hjort, N.; Jones, M. (1998), Robust and eÔ¨Écient estimation by minimising a density power divergence. Biometrika , 85, 187 549‚Äì559. 188 [2] Bowman, A. W. (1984), An alternative method of cross-validation for the smoothing of density estimates, Biometrika 71, 353‚Äì360. 189 [3] Bowman, A. W. (1985), A comparative study of some kernel-based nonparametric density estimators, Journal of Statistical Computation and 190 Simulation, 21, 3-4, 313‚Äì327. 191 [4] Copas, J. P. and M. J. Fryer (1980), Density estimation and suicide risks in psychiatric treatment, Journal of the Royal Statistical Society, 192 Series B 143, 167‚Äì176. 193 [5] Hall, P. (1983). Large sampie optimality of least squares cross-validation in density estirnation, Ann. Statist., 11, 1156‚Äì1174. 194 [6] Hall, P. (1985). Asymptotic theory of minimum integrated square error for multivariate density estimation. In Multivariate Analysis VI, ed. 195 P.R. Krishnaiah, Elsevier Science, Amsterdam, 289‚Äì309. 196 [7] Hall, P. and J. S. Marron (1991), Local minima in cross-validation functions, Journal of the Royal Statistical Society, Series B 53, 245‚Äì252. 197 [8] Hrdle, W. (1991), Smoothing techniques with implementation in S, Springer-Verlag, New York. 198 [9] Jones,M. C., Marron, J. S., and Sheather, S. J. (1996), A brief survey of bandwidth selection for density estimation, Journal of the American 199 Statistical Association, 91, 433, 401‚Äì407,. 200 [10] Jones, M. C. and R. F. Kappenman (1991), On a class of kernel density estimate bandwidth selectors, Scandinavian Journal of Statistics 19, 201 337?349. 202 [11] Kanazawa, Y. (1993), Hellinger distance and Kullback-Leibler loss for the kernel density estimator. Statistics and Probability Letters 18 203 315‚Äì321 204 [12] Loader, C. R. (1999), ?Bandwidth selection: classical or plug-in?? The Annals of Statistics, 27, 2, 415‚Äì438, . 205 [13] Marron, J. S. and M. P. Wand (1992), Exact mean integrated squared error, Annals of Statistics 20, 712‚Äì736. 206 [14] Minami, M.; Eguchi, S. Robust blind source separation by Beta-divergence. Neural Comput. 2002, 13, 1859‚Äì1886. 207 [15] Park, B, U., Turlach, B, S. (1992) Practical performance of several data driven bandwidth selectors, Computational Statistics,7, 251‚Äì270 208 [16] Parzen, E. (1962), On estimation of a probability density function and mode, Annals of Mathematical Statistics 33, 1065‚Äì1076. 209 [17] Rudemo, M. (1982), Empirical choice of histograms and kernel density estimators, Scandinavia Journal of Statistics 9, 65‚Äì78. 210 [18] Scott, W. D (1992), Multivariate density estimation theory, practice, and visualization, Wiley, New York. 211 [19] Scott, D. W. and Terrell, G. R. (1987), Biased and unbiased cross-validation in density estimation, Journal of the American Statistical 212 Association 82, 1131‚Äì1146. 213 [20] Sheather, S. J. and Jones, M. C. (1991), A reliable data-based bandwidth selection method for kernel density estimation, Journal of the Royal 214 Statistical Society series B 53, 683‚Äì690. 215 [21] Silverman, B. W. (1986), Density estimation for statistics and data analysis, Chapman and Hall, London. 216 [22] Stone, C. J. (1984), An asymptotically optimal window selection rule for kernel density estimates, Annals of Statistics, 12, 1285‚Äì1297. 217 [23] Venables, W. N. and B. D. Ripley (2002), Modern applied statistics with S 4th Edition, Springer, New York. 218 [24] Wand, M. P. and M. C. Jones (1995), Kernel Smoothing, Chapman and Hall, London, UK. 219 [25] Weisberg, S. (1980), Applied linear regression, Wiley, New York. 220 [26] Zhang, J. (2015), Generalized least squares cross-validation in kernel density estimation,Statistica Neerlandica, 69, 3, 315‚Äì328. 221 11 Table 1: RE bh  for normal mixture f(x) = 0.5œÜ(x) + 0.5œÜœÉ(x ‚àí ¬µ) n bhNR bhLSCV bhS J bhLSCV4 bhD1.1CV bhD1.5CV bhD1.8CV bhD4CV ¬µ = 0 œÉ = 1 50 0.877 0.633 0.798 0.909 0.768 0.716 0.694 0.770 200 0.928 0.730 0.867 0.944 0.978 0.944 0.924 0.833 700 0.974 0.767 0.942 0.956 0.978 0.981 0.973 0.959 ¬µ = 0 œÉ = 0.5 50 0899 0.633 0.850 0.776 0.600 0.900 0.917 0.502 200 0.948 0.644 0.895 0.908 0.884 0.987 0.949 0.709 700 0.952 0.767 0.939 0.914 0.980 0.821 0.816 0.967 ¬µ = 0 œÉ = 0.1 50 0.562 0.781 0.779 0.748 0.700 0.618 0.573 0.670 200 0.550 0.810 0.902 0.891 0.930 0.890 0.747 0.834 700 0.535 0.872 0.966 0.957 0.993 0.977 0.963 0.920 ¬µ = 1 œÉ = 1 50 0.871 0.585 0.791 0.904 0.665 0.830 0.855 0.680 200 0.955 0.699 0.905 0.949 0.928 0.989 0.992 0.892 700 0.977 0.763 0.943 0.960 0.981 0.973 0.977 0.975 ¬µ = 1 œÉ = 0.5 50 0.855 0.562 0.856 0.806 0.749 0.751 0.906 0.439 200 0.867 0.760 0.929 0.874 0.965 0.975 0.989 0.690 700 0.823 0.831 0.955 0.922 0.942 0.923 0.720 0.902 ¬µ = 1 œÉ = .1 50 0.2370 0.781 0.439 0.755 0.544 0.606 0.531 0.528 200 0.1010 0.810 0.419 0.904 0.756 0.834 0.717 0.716 700 0.0503 0.912 0.557 0.949 0.968 0.980 0.921 0.899 ¬µ = 5 œÉ = 1 50 0.411 0.741 0.875 0.864 0.618 0.883 0.490 0.528 200 0.285 0.861 0.947 0.948 0.852 0.981 0.688 0.716 700 0.215 0.876 0.972 0.967 0.997 0.870 0.914 0.979 ¬µ = 5 œÉ = 0.5 50 0.2390 0.707 0.696 0.871 0.657 0.871 0.806 0.460 200 0.1330 0.760 0.747 0.923 0.907 0.986 0.983 0.726 700 0.0804 0.842 0.846 0.957 0.990 0.800 0.912 0.858 ¬µ = 5 œÉ = 0.1 50 0.1360 0.588 0.1760 0.791 0.630 0.563 0.693 0.640 200 0.0523 0.458 0.0946 0.905 0.878 0.774 0.940 0.858 700 0.0205 0.341 0.0665 0.969 0.968 0.955 0.989 0.980 12 Table 2: E bh  for normal mixture f(x) = 0.5œÜ(x) + 0.5œÜœÉ(x ‚àí ¬µ) n bhNR bhLSCV bhS J bhLSCV4 bhD1.1CV bhD1.5CV bhD1.8CV bhD4CV hMIS E ¬µ = 0 œÉ = 1 50 0.455 0.480 0.452 0.516 0.323 0.330 0.379 0.360 0.520 200 0.354 0.373 0.349 0.385 0.321 0.328 0.376 0.348 0.383 700 0.283 0.288 0.282 0.301 0.308 0.309 0.307 0.308 0.293 ¬µ = 0 œÉ = 0.5 50 0.318 0.362 0.306 0.382 0.223 0.286 0.286 0.174 0.343 200 0.250 0.262 0.231 0.266 0.193 0.280 0.285 0.168 0.248 700 0.197 0.191 0.184 0.200 0.186 0.244 0.245 0.166 0.188 ¬µ = 0 œÉ = 0.1 50 0.1320 0.0897 0.0915 0.1040 0.510 0.0409 0.0356 0.0403 0.0752 200 0.0887 0.0557 0.0572 0.0597 0.485 0.0377 0.0331 0.0400 0.0530 700 0.0668 0.0406 0.0406 0.0422 0.421 0.0370 0.0329 0.0403 0.0398 ¬µ = 1 œÉ = 1 50 0.510 0.569 0.511 0.573 0.429 0.426 0.435 0.360 0.373 200 0.403 0.418 0.403 0.440 0.395 0.423 0.429 0.359 0.265 700 0.317 0.329 0.315 0.337 0.354 0.345 0.346 0.341 0.199 ¬µ = 1 œÉ = 0.5 50 0.409 0.395 0.366 0.446 0.326 0.342 0.283 0.155 0.373 200 0.328 0.278 0.270 0.296 0.280 0.282 0.283 0.152 0.265 700 0.256 0.197 0.203 0.212 0.233 0.239 0.281 0.151 0.199 ¬µ = 1 œÉ = .1 50 0.351 0.0877 0.1780 0.1030 0.0422 0.0451 0.0523 0.0413 0.0752 200 0.280 0.0553 0.1050 0.0614 0.0380 0.0380 0.0401 0.0357 0.0530 700 0.222 0.0408 0.0657 0.0426 0.0343 0.0314 0.0311 0.0309 0.0398 ¬µ = 5 œÉ = 1 50 1.300 0.636 0.753 0.742 0.420 0.475 0.315 0.308 0.608 200 0.986 0.456 0.495 0.472 0.330 0.470 0.285 0.276 0.441 700 0.770 0.342 0.361 0.261 0.275 0.336 ¬µ = 5 œÉ = 0.5 50 1.260 0.407 0.590 0.448 0.310 0.295 0.251 0.217 0.369 200 0.963 0.274 0.378 0.296 0.210 0.286 0.250 0.171 0.262 700 0.750 0.201 0.255 0.210 0.209 0.270 0.244 0.146 0.197 ¬µ = 5 œÉ = 0.1 50 1.260 0.0871 0.523 0.1040 0.045 0.0415 0.0476 0.0497 0.0752 200 0.956 0.0560 0.292 0.0603 0.040 0.0385 0.0427 0.0395 0.0530 700 0.745 0.0406 0.172 0.0420 0.039 0.0339 0.0426 0.0387 0.0398 13 Table 3: E|bh/hMIS E ‚àí 1| for normal mixture f(x) = 0.5œÜ(x) + 0.5œÜœÉ(x ‚àí ¬µ) n bhNR bhLSCV bhS J bhLSCV4 bhD1.1CV bhD1.5CV bhD1.8CV bhD4CV ¬µ = 0 œÉ = 1 50 0.1350 0.1240 0.1640 0.0874 0.3790 0.3650 0.2710 0.328 200 0.0785 0.0829 0.1050 0.0578 0.1620 0.1390 0.0177 0.145 700 0.0396 0.0717 0.0572 0.0436 0.0509 0.0531 0.0486 0.052 ¬µ = 0 œÉ = 0.5 50 0.1370 0.1510 0.1670 0.1510 0.4560 0.179 0.166 0.342 200 0.0655 0.1360 0.0882 0.1010 0.2490 0.135 0.150 0.264 700 0.0559 0.0729 0.0537 0.0818 0.0104 0.299 0.300 0.188 ¬µ = 0 œÉ = 0.1 50 0.772 0.2530 0.3000 0.3990 0.4410 0.4980 0.562 0.463 200 0.674 0.1210 0.1250 0.1520 0.2070 0.2870 0.378 0.244 700 0.679 0.0772 0.0506 0.0726 0.585 0.0503 0.171 0.015 ¬µ = 1 œÉ = 1 50 0.1430 0.1040 0.1630 0.0748 0.398 0.2760 0.2610 0.387 200 0.0774 0.0800 0.0931 0.0501 0.184 0.0249 0.0124 0.171 700 0.0483 0.0626 0.0600 0.0361 0.037 0.0426 0.0416 0.0466 ¬µ = 1 œÉ = 0.5 50 0.172 0.1930 0.1400 0.2260 0.4560 0.3580 0.2420 0.584 200 0.236 0.1530 0.0899 0.1460 0.121 0.0965 0.0668 0.415 700 0.285 0.0989 0.0506 0.0794 0.169 0.2010 0.4100 0.223 ¬µ = 1 œÉ = .1 50 3.67 0.2620 1.380 0.3980 0.544 0.507 0.586 0.589 200 4.29 0.1250 0.986 0.1720 0.353 0.300 0.413 0.416 700 4.58 0.0838 0.652 0.0878 0.137 0.068 0.218 0.221 ¬µ = 5 œÉ = 1 50 1.14 0.1450 0.2390 0.2430 0.4580 0.2840 0.570 0.546 200 1.23 0.0815 0.1210 0.0899 0.2530 0.0147 0.409 0.376 700 1.29 0.0686 0.0745 0.0540 0.0203 0.2970 0.224 0.179 ¬µ = 5 œÉ = 0.5 50 2.40 0.1860 0.600 0.2510 0.4340 0.2660 0.3380 0.6033 200 2.68 0.1180 0.444 0.1440 0.2020 0.0315 0.0673 0.440 700 2.80 0.0743 0.296 0.0804 0.0597 0.3800 0.2390 0.237 ¬µ = 5 œÉ = 0.1 50 15.7 0.884 5.95 0.3980 0.4810 0.549 0.433 0.484 200 17.1 1.021 4.51 0.1570 0.2630 0.359 0.194 0.267 700 17.7 1.104 3.34 0.0656 0.0178 0.146 0.074 0.0573 14 